{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec2225d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Setup complete\n"
     ]
    }
   ],
   "source": [
    "# Core libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, CosineAnnealingWarmRestarts\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "def set_seed(seed=42):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc7ff06",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrajectoryTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Advanced Transformer model for multi-agent trajectory prediction\n",
    "    with attention mechanisms and uncertainty estimation\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 input_dim=64,           # Feature dimension per player\n",
    "                 hidden_dim=256,         # Hidden layer size\n",
    "                 num_heads=8,            # Attention heads\n",
    "                 num_layers=6,           # Transformer layers\n",
    "                 max_players=22,         # Max players on field\n",
    "                 max_seq_len=50,         # Max input frames\n",
    "                 max_output_frames=35,   # Max frames to predict\n",
    "                 dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.max_output_frames = max_output_frames\n",
    "        \n",
    "        # Input projection\n",
    "        self.input_projection = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "        # Positional encoding for temporal information\n",
    "        self.temporal_encoding = nn.Parameter(\n",
    "            self._generate_positional_encoding(max_seq_len, hidden_dim)\n",
    "        )\n",
    "        \n",
    "        # Player role embedding (Passer, Receiver, Route Runner, Coverage, Other)\n",
    "        self.role_embedding = nn.Embedding(5, hidden_dim)\n",
    "        \n",
    "        # Side embedding (Offense/Defense)\n",
    "        self.side_embedding = nn.Embedding(2, hidden_dim)\n",
    "        \n",
    "        # Transformer encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=hidden_dim,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=hidden_dim * 4,\n",
    "            dropout=dropout,\n",
    "            activation='gelu',\n",
    "            batch_first=True,\n",
    "            norm_first=True\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            encoder_layer, \n",
    "            num_layers=num_layers\n",
    "        )\n",
    "        \n",
    "        # Cross-attention for player interactions\n",
    "        self.cross_attention = nn.MultiheadAttention(\n",
    "            embed_dim=hidden_dim,\n",
    "            num_heads=num_heads,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # Trajectory decoder with residual connections\n",
    "        self.trajectory_decoder = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(hidden_dim, hidden_dim),\n",
    "                nn.LayerNorm(hidden_dim),\n",
    "                nn.GELU(),\n",
    "                nn.Dropout(dropout)\n",
    "            ) for _ in range(3)\n",
    "        ])\n",
    "        \n",
    "        # Output head - predicts variable length trajectories\n",
    "        self.output_head = nn.Linear(hidden_dim, max_output_frames * 2)\n",
    "        \n",
    "        # Uncertainty estimation head\n",
    "        self.uncertainty_head = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_dim // 2, max_output_frames * 2)\n",
    "        )\n",
    "        \n",
    "        # Frame-specific output projection\n",
    "        self.frame_predictor = nn.Linear(hidden_dim, 2)\n",
    "        \n",
    "    def _generate_positional_encoding(self, max_len, d_model):\n",
    "        \"\"\"Generate sinusoidal positional encoding\"\"\"\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-np.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(1, max_len, d_model)\n",
    "        pe[0, :, 0::2] = torch.sin(position * div_term)\n",
    "        pe[0, :, 1::2] = torch.cos(position * div_term)\n",
    "        return pe\n",
    "        \n",
    "    def forward(self, x, role_ids, side_ids, num_output_frames, mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: [batch, seq_len, num_players, input_dim]\n",
    "            role_ids: [batch, num_players] - player role IDs (0-4)\n",
    "            side_ids: [batch, num_players] - player side IDs (0=offense, 1=defense)\n",
    "            num_output_frames: [batch] - number of frames to predict per sample\n",
    "            mask: [batch, seq_len, num_players] - attention mask\n",
    "        Returns:\n",
    "            trajectories: [batch, num_players, max_output_frames, 2]\n",
    "            uncertainties: [batch, num_players, max_output_frames, 2]\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, num_players, _ = x.shape\n",
    "        \n",
    "        # Reshape for processing: [B*P, T, D]\n",
    "        x = x.reshape(batch_size * num_players, seq_len, self.input_dim)\n",
    "        \n",
    "        # Project input\n",
    "        x = self.input_projection(x)\n",
    "        \n",
    "        # Add temporal encoding\n",
    "        x = x + self.temporal_encoding[:, :seq_len, :]\n",
    "        \n",
    "        # Add role embedding\n",
    "        role_emb = self.role_embedding(role_ids)  # [B, P, H]\n",
    "        role_emb = role_emb.reshape(batch_size * num_players, 1, self.hidden_dim)\n",
    "        x = x + role_emb\n",
    "        \n",
    "        # Add side embedding\n",
    "        side_emb = self.side_embedding(side_ids)  # [B, P, H]\n",
    "        side_emb = side_emb.reshape(batch_size * num_players, 1, self.hidden_dim)\n",
    "        x = x + side_emb\n",
    "        \n",
    "        # Apply transformer encoder\n",
    "        if mask is not None:\n",
    "            mask = mask.reshape(batch_size * num_players, seq_len)\n",
    "        encoded = self.transformer_encoder(x, src_key_padding_mask=mask)\n",
    "        \n",
    "        # Reshape back for cross-attention: [B, P, T, H]\n",
    "        encoded = encoded.reshape(batch_size, num_players, seq_len, self.hidden_dim)\n",
    "        \n",
    "        # Apply cross-attention across players (at last timestep)\n",
    "        last_encoded = encoded[:, :, -1, :]  # [B, P, H]\n",
    "        attended, _ = self.cross_attention(last_encoded, last_encoded, last_encoded)\n",
    "        \n",
    "        # Residual connection\n",
    "        last_encoded = last_encoded + attended\n",
    "        \n",
    "        # Apply decoder layers with residual connections\n",
    "        hidden = last_encoded\n",
    "        for decoder_layer in self.trajectory_decoder:\n",
    "            hidden = hidden + decoder_layer(hidden)\n",
    "        \n",
    "        # Predict trajectories\n",
    "        traj_flat = self.output_head(hidden)  # [B, P, max_T*2]\n",
    "        trajectories = traj_flat.reshape(\n",
    "            batch_size, num_players, self.max_output_frames, 2\n",
    "        )\n",
    "        \n",
    "        # Predict uncertainties (log variance)\n",
    "        uncert_flat = self.uncertainty_head(hidden)\n",
    "        uncertainties = F.softplus(uncert_flat).reshape(\n",
    "            batch_size, num_players, self.max_output_frames, 2\n",
    "        )\n",
    "        \n",
    "        return trajectories, uncertainties\n",
    "\n",
    "print(\"âœ… Transformer model defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c8e736",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NFLTrajectoryDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for loading NFL tracking data for trajectory prediction\n",
    "    Handles variable sequence lengths and player counts\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 weeks,  # List of week numbers to include\n",
    "                 data_dir='.',\n",
    "                 supplementary_file='supplementary_data.csv',\n",
    "                 max_input_frames=40,\n",
    "                 max_players=22,\n",
    "                 normalize=True,\n",
    "                 augment=False):\n",
    "        \n",
    "        self.max_input_frames = max_input_frames\n",
    "        self.max_players = max_players\n",
    "        self.normalize = normalize\n",
    "        self.augment = augment\n",
    "        \n",
    "        print(f\"Loading data for weeks: {weeks}\")\n",
    "        \n",
    "        # Load input data\n",
    "        input_files = [f'input_2023_w{w:02d}.csv' for w in weeks]\n",
    "        self.input_data = pd.concat([\n",
    "            pd.read_csv(f) for f in input_files if Path(f).exists()\n",
    "        ], ignore_index=True)\n",
    "        \n",
    "        # Load output data\n",
    "        output_files = [f'output_2023_w{w:02d}.csv' for w in weeks]\n",
    "        self.output_data = pd.concat([\n",
    "            pd.read_csv(f) for f in output_files if Path(f).exists()\n",
    "        ], ignore_index=True)\n",
    "        \n",
    "        # Load supplementary data\n",
    "        self.supplementary = pd.read_csv(supplementary_file)\n",
    "        \n",
    "        print(f\"  Input data: {len(self.input_data):,} rows\")\n",
    "        print(f\"  Output data: {len(self.output_data):,} rows\")\n",
    "        \n",
    "        # Get unique plays\n",
    "        self.plays = self.input_data.groupby(['game_id', 'play_id']).size().reset_index()[['game_id', 'play_id']]\n",
    "        print(f\"  Total plays: {len(self.plays):,}\")\n",
    "        \n",
    "        # Define feature columns\n",
    "        self.feature_cols = [\n",
    "            'x', 'y', 's', 'a', 'dir', 'o',\n",
    "            'absolute_yardline_number'\n",
    "        ]\n",
    "        \n",
    "        # Add engineered features if they exist\n",
    "        engineered_features = [\n",
    "            'distance_to_ball_land', 'angle_to_ball_land',\n",
    "            'velocity_x', 'velocity_y',\n",
    "            'distance_to_qb', 'distance_to_los'\n",
    "        ]\n",
    "        for feat in engineered_features:\n",
    "            if feat in self.input_data.columns:\n",
    "                self.feature_cols.append(feat)\n",
    "        \n",
    "        print(f\"  Using {len(self.feature_cols)} features: {self.feature_cols}\")\n",
    "        \n",
    "        # Compute normalization stats\n",
    "        if self.normalize:\n",
    "            self.compute_normalization_stats()\n",
    "        \n",
    "        # Role mapping\n",
    "        self.role_map = {\n",
    "            'Passer': 0,\n",
    "            'Targeted Receiver': 1,\n",
    "            'Other Route Runner': 2,\n",
    "            'Defensive Coverage': 3\n",
    "        }\n",
    "        \n",
    "        # Side mapping\n",
    "        self.side_map = {\n",
    "            'Offense': 0,\n",
    "            'Defense': 1\n",
    "        }\n",
    "    \n",
    "    def compute_normalization_stats(self):\n",
    "        \"\"\"Compute mean and std for normalization\"\"\"\n",
    "        print(\"  Computing normalization statistics...\")\n",
    "        self.feature_means = {}\n",
    "        self.feature_stds = {}\n",
    "        \n",
    "        for col in self.feature_cols:\n",
    "            if col in self.input_data.columns:\n",
    "                self.feature_means[col] = self.input_data[col].mean()\n",
    "                self.feature_stds[col] = self.input_data[col].std()\n",
    "            else:\n",
    "                self.feature_means[col] = 0.0\n",
    "                self.feature_stds[col] = 1.0\n",
    "        \n",
    "        print(\"  âœ… Normalization stats computed\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.plays)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Returns a single play with input and output sequences\n",
    "        \"\"\"\n",
    "        game_id = self.plays.iloc[idx]['game_id']\n",
    "        play_id = self.plays.iloc[idx]['play_id']\n",
    "        \n",
    "        # Get input data for this play\n",
    "        input_play = self.input_data[\n",
    "            (self.input_data['game_id'] == game_id) & \n",
    "            (self.input_data['play_id'] == play_id)\n",
    "        ].copy()\n",
    "        \n",
    "        # Get output data for this play\n",
    "        output_play = self.output_data[\n",
    "            (self.output_data['game_id'] == game_id) & \n",
    "            (self.output_data['play_id'] == play_id)\n",
    "        ].copy()\n",
    "        \n",
    "        # Sort by frame and player\n",
    "        input_play = input_play.sort_values(['frame_id', 'nfl_id'])\n",
    "        output_play = output_play.sort_values(['frame_id', 'nfl_id'])\n",
    "        \n",
    "        # Get unique frames and players\n",
    "        frames = sorted(input_play['frame_id'].unique())\n",
    "        players = sorted(input_play['nfl_id'].unique())\n",
    "        \n",
    "        num_frames = min(len(frames), self.max_input_frames)\n",
    "        num_players = min(len(players), self.max_players)\n",
    "        \n",
    "        # Initialize tensors\n",
    "        input_seq = np.zeros((num_frames, num_players, len(self.feature_cols)))\n",
    "        mask = np.ones((num_frames, num_players), dtype=bool)\n",
    "        \n",
    "        # Role and side IDs\n",
    "        role_ids = np.zeros(num_players, dtype=np.int64)\n",
    "        side_ids = np.zeros(num_players, dtype=np.int64)\n",
    "        players_to_predict = np.zeros(num_players, dtype=bool)\n",
    "        \n",
    "        # Fill input sequence\n",
    "        for f_idx, frame in enumerate(frames[:num_frames]):\n",
    "            frame_data = input_play[input_play['frame_id'] == frame]\n",
    "            \n",
    "            for p_idx, player in enumerate(players[:num_players]):\n",
    "                player_data = frame_data[frame_data['nfl_id'] == player]\n",
    "                \n",
    "                if len(player_data) > 0:\n",
    "                    row = player_data.iloc[0]\n",
    "                    \n",
    "                    # Extract features\n",
    "                    for feat_idx, col in enumerate(self.feature_cols):\n",
    "                        if col in player_data.columns:\n",
    "                            input_seq[f_idx, p_idx, feat_idx] = row[col]\n",
    "                    \n",
    "                    mask[f_idx, p_idx] = False\n",
    "                    \n",
    "                    # Get role and side (only need to do once)\n",
    "                    if f_idx == 0:\n",
    "                        role = row.get('player_role', 'Other')\n",
    "                        role_ids[p_idx] = self.role_map.get(role, 3)\n",
    "                        \n",
    "                        side = row.get('player_side', 'Defense')\n",
    "                        side_ids[p_idx] = self.side_map.get(side, 1)\n",
    "                        \n",
    "                        players_to_predict[p_idx] = row.get('player_to_predict', False)\n",
    "        \n",
    "        # Normalize features\n",
    "        if self.normalize:\n",
    "            for feat_idx, col in enumerate(self.feature_cols):\n",
    "                mean = self.feature_means.get(col, 0.0)\n",
    "                std = self.feature_stds.get(col, 1.0)\n",
    "                input_seq[:, :, feat_idx] = (input_seq[:, :, feat_idx] - mean) / (std + 1e-8)\n",
    "        \n",
    "        # Data augmentation (flip field)\n",
    "        if self.augment and np.random.rand() > 0.5:\n",
    "            # Flip y-coordinates\n",
    "            y_idx = self.feature_cols.index('y')\n",
    "            input_seq[:, :, y_idx] = 53.3 - input_seq[:, :, y_idx]\n",
    "            \n",
    "            # Flip angles\n",
    "            if 'dir' in self.feature_cols:\n",
    "                dir_idx = self.feature_cols.index('dir')\n",
    "                input_seq[:, :, dir_idx] = (360 - input_seq[:, :, dir_idx]) % 360\n",
    "            if 'o' in self.feature_cols:\n",
    "                o_idx = self.feature_cols.index('o')\n",
    "                input_seq[:, :, o_idx] = (360 - input_seq[:, :, o_idx]) % 360\n",
    "        \n",
    "        # Extract output trajectories\n",
    "        output_frames = sorted(output_play['frame_id'].unique())\n",
    "        num_output_frames = len(output_frames)\n",
    "        \n",
    "        # Initialize output with zeros (will be masked during loss computation)\n",
    "        output_seq = np.zeros((num_players, 35, 2))  # Max 35 frames\n",
    "        \n",
    "        for f_idx, frame in enumerate(output_frames):\n",
    "            frame_data = output_play[output_play['frame_id'] == frame]\n",
    "            \n",
    "            for p_idx, player in enumerate(players[:num_players]):\n",
    "                player_data = frame_data[frame_data['nfl_id'] == player]\n",
    "                \n",
    "                if len(player_data) > 0:\n",
    "                    output_seq[p_idx, f_idx, 0] = player_data.iloc[0]['x']\n",
    "                    output_seq[p_idx, f_idx, 1] = player_data.iloc[0]['y']\n",
    "        \n",
    "        # Get supplementary info\n",
    "        supp = self.supplementary[\n",
    "            (self.supplementary['game_id'] == game_id) & \n",
    "            (self.supplementary['play_id'] == play_id)\n",
    "        ]\n",
    "        \n",
    "        metadata = {\n",
    "            'game_id': game_id,\n",
    "            'play_id': play_id,\n",
    "            'num_output_frames': num_output_frames,\n",
    "            'num_players': num_players,\n",
    "            'num_frames': num_frames\n",
    "        }\n",
    "        \n",
    "        if len(supp) > 0:\n",
    "            supp_row = supp.iloc[0]\n",
    "            metadata['pass_result'] = supp_row.get('pass_result', 'Unknown')\n",
    "            metadata['coverage_type'] = supp_row.get('team_coverage_type', 'Unknown')\n",
    "        \n",
    "        return {\n",
    "            'input_seq': torch.FloatTensor(input_seq),\n",
    "            'output_seq': torch.FloatTensor(output_seq),\n",
    "            'mask': torch.BoolTensor(mask),\n",
    "            'role_ids': torch.LongTensor(role_ids),\n",
    "            'side_ids': torch.LongTensor(side_ids),\n",
    "            'players_to_predict': torch.BoolTensor(players_to_predict),\n",
    "            'num_output_frames': num_output_frames,\n",
    "            'metadata': metadata\n",
    "        }\n",
    "\n",
    "print(\"âœ… Dataset class defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c494534c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define week splits for cross-validation\n",
    "# Strategy: Use weeks as folds to maintain temporal structure\n",
    "\n",
    "# Option 1: Simple train/val split\n",
    "train_weeks = list(range(1, 15))  # Weeks 1-14 for training\n",
    "val_weeks = [15, 16, 17, 18]      # Weeks 15-18 for validation\n",
    "\n",
    "print(\"Creating datasets...\")\n",
    "train_dataset = NFLTrajectoryDataset(\n",
    "    weeks=train_weeks,\n",
    "    normalize=True,\n",
    "    augment=True  # Enable augmentation for training\n",
    ")\n",
    "\n",
    "val_dataset = NFLTrajectoryDataset(\n",
    "    weeks=val_weeks,\n",
    "    normalize=True,\n",
    "    augment=False  # No augmentation for validation\n",
    ")\n",
    "\n",
    "# Copy normalization stats from train to val\n",
    "val_dataset.feature_means = train_dataset.feature_means\n",
    "val_dataset.feature_stds = train_dataset.feature_stds\n",
    "\n",
    "print(f\"\\nðŸ“Š Dataset Summary:\")\n",
    "print(f\"  Training plays: {len(train_dataset):,}\")\n",
    "print(f\"  Validation plays: {len(val_dataset):,}\")\n",
    "print(f\"  Total plays: {len(train_dataset) + len(val_dataset):,}\")\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 16  # Adjust based on GPU memory\n",
    "num_workers = 4  # Adjust based on CPU cores\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True if torch.cuda.is_available() else False,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True if torch.cuda.is_available() else False\n",
    ")\n",
    "\n",
    "print(f\"\\nðŸ“¦ DataLoader Summary:\")\n",
    "print(f\"  Training batches: {len(train_loader):,}\")\n",
    "print(f\"  Validation batches: {len(val_loader):,}\")\n",
    "print(f\"  Batch size: {batch_size}\")\n",
    "\n",
    "# Test loading a batch\n",
    "print(\"\\nðŸ§ª Testing data loading...\")\n",
    "test_batch = next(iter(train_loader))\n",
    "print(f\"  Input shape: {test_batch['input_seq'].shape}\")\n",
    "print(f\"  Output shape: {test_batch['output_seq'].shape}\")\n",
    "print(f\"  Role IDs shape: {test_batch['role_ids'].shape}\")\n",
    "print(f\"  âœ… Data loading successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234a5313",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrajectoryLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Custom loss function for trajectory prediction\n",
    "    Combines MSE with uncertainty weighting\n",
    "    \"\"\"\n",
    "    def __init__(self, uncertainty_weight=0.1):\n",
    "        super().__init__()\n",
    "        self.uncertainty_weight = uncertainty_weight\n",
    "    \n",
    "    def forward(self, pred_traj, pred_uncert, target_traj, players_to_predict, num_output_frames):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            pred_traj: [B, P, T, 2] - predicted trajectories\n",
    "            pred_uncert: [B, P, T, 2] - predicted uncertainties\n",
    "            target_traj: [B, P, T, 2] - ground truth trajectories\n",
    "            players_to_predict: [B, P] - mask for which players to evaluate\n",
    "            num_output_frames: [B] - actual number of frames per sample\n",
    "        \"\"\"\n",
    "        batch_size = pred_traj.shape[0]\n",
    "        total_loss = 0.0\n",
    "        \n",
    "        for b in range(batch_size):\n",
    "            # Get actual frames for this sample\n",
    "            T = num_output_frames[b]\n",
    "            \n",
    "            # Get players to predict\n",
    "            player_mask = players_to_predict[b]\n",
    "            \n",
    "            if player_mask.sum() == 0:\n",
    "                continue\n",
    "            \n",
    "            # Extract relevant predictions and targets\n",
    "            pred = pred_traj[b, player_mask, :T, :]\n",
    "            uncert = pred_uncert[b, player_mask, :T, :]\n",
    "            target = target_traj[b, player_mask, :T, :]\n",
    "            \n",
    "            # Compute MSE loss\n",
    "            mse_loss = F.mse_loss(pred, target)\n",
    "            \n",
    "            # Uncertainty-weighted loss (negative log likelihood)\n",
    "            # loss = 0.5 * (error^2 / variance + log(variance))\n",
    "            error_sq = (pred - target) ** 2\n",
    "            nll_loss = 0.5 * (error_sq / (uncert + 1e-6) + torch.log(uncert + 1e-6))\n",
    "            nll_loss = nll_loss.mean()\n",
    "            \n",
    "            # Combine losses\n",
    "            total_loss += mse_loss + self.uncertainty_weight * nll_loss\n",
    "        \n",
    "        return total_loss / batch_size\n",
    "\n",
    "print(\"âœ… Loss function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8c78cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, optimizer, criterion, device, epoch):\n",
    "    \"\"\"Train for one epoch\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    num_batches = 0\n",
    "    \n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch} [Train]\")\n",
    "    \n",
    "    for batch in pbar:\n",
    "        # Move data to device\n",
    "        input_seq = batch['input_seq'].to(device)\n",
    "        output_seq = batch['output_seq'].to(device)\n",
    "        mask = batch['mask'].to(device)\n",
    "        role_ids = batch['role_ids'].to(device)\n",
    "        side_ids = batch['side_ids'].to(device)\n",
    "        players_to_predict = batch['players_to_predict'].to(device)\n",
    "        num_output_frames = batch['num_output_frames']\n",
    "        \n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        pred_traj, pred_uncert = model(\n",
    "            input_seq, \n",
    "            role_ids, \n",
    "            side_ids, \n",
    "            num_output_frames,\n",
    "            mask=mask\n",
    "        )\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = criterion(\n",
    "            pred_traj, \n",
    "            pred_uncert, \n",
    "            output_seq, \n",
    "            players_to_predict,\n",
    "            num_output_frames\n",
    "        )\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Track loss\n",
    "        total_loss += loss.item()\n",
    "        num_batches += 1\n",
    "        \n",
    "        # Update progress bar\n",
    "        pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "    \n",
    "    return total_loss / num_batches\n",
    "\n",
    "\n",
    "def validate_epoch(model, val_loader, criterion, device, epoch):\n",
    "    \"\"\"Validate for one epoch\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    num_batches = 0\n",
    "    \n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "    all_metadata = []\n",
    "    \n",
    "    pbar = tqdm(val_loader, desc=f\"Epoch {epoch} [Val]\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in pbar:\n",
    "            # Move data to device\n",
    "            input_seq = batch['input_seq'].to(device)\n",
    "            output_seq = batch['output_seq'].to(device)\n",
    "            mask = batch['mask'].to(device)\n",
    "            role_ids = batch['role_ids'].to(device)\n",
    "            side_ids = batch['side_ids'].to(device)\n",
    "            players_to_predict = batch['players_to_predict'].to(device)\n",
    "            num_output_frames = batch['num_output_frames']\n",
    "            \n",
    "            # Forward pass\n",
    "            pred_traj, pred_uncert = model(\n",
    "                input_seq, \n",
    "                role_ids, \n",
    "                side_ids, \n",
    "                num_output_frames,\n",
    "                mask=mask\n",
    "            )\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = criterion(\n",
    "                pred_traj, \n",
    "                pred_uncert, \n",
    "                output_seq, \n",
    "                players_to_predict,\n",
    "                num_output_frames\n",
    "            )\n",
    "            \n",
    "            # Track loss\n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "            \n",
    "            # Store predictions for analysis\n",
    "            all_predictions.append(pred_traj.cpu())\n",
    "            all_targets.append(output_seq.cpu())\n",
    "            all_metadata.append(batch['metadata'])\n",
    "            \n",
    "            # Update progress bar\n",
    "            pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "    \n",
    "    return total_loss / num_batches, all_predictions, all_targets, all_metadata\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, num_epochs=50, lr=1e-4, device='cuda'):\n",
    "    \"\"\"\n",
    "    Complete training loop with validation\n",
    "    \"\"\"\n",
    "    print(f\"\\nðŸš€ Starting training for {num_epochs} epochs\")\n",
    "    print(f\"   Device: {device}\")\n",
    "    print(f\"   Learning rate: {lr}\")\n",
    "    \n",
    "    # Move model to device\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer = AdamW(\n",
    "        model.parameters(), \n",
    "        lr=lr, \n",
    "        weight_decay=1e-5,\n",
    "        betas=(0.9, 0.999)\n",
    "    )\n",
    "    \n",
    "    # Learning rate scheduler\n",
    "    scheduler = ReduceLROnPlateau(\n",
    "        optimizer, \n",
    "        mode='min', \n",
    "        factor=0.5, \n",
    "        patience=5, \n",
    "        verbose=True,\n",
    "        min_lr=1e-7\n",
    "    )\n",
    "    \n",
    "    # Loss function\n",
    "    criterion = TrajectoryLoss(uncertainty_weight=0.1)\n",
    "    \n",
    "    # Training history\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'val_loss': [],\n",
    "        'learning_rates': []\n",
    "    }\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    max_patience = 10\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        # Train\n",
    "        train_loss = train_epoch(model, train_loader, optimizer, criterion, device, epoch)\n",
    "        \n",
    "        # Validate\n",
    "        val_loss, val_preds, val_targets, val_metadata = validate_epoch(\n",
    "            model, val_loader, criterion, device, epoch\n",
    "        )\n",
    "        \n",
    "        # Update scheduler\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        # Get current learning rate\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        \n",
    "        # Save history\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['learning_rates'].append(current_lr)\n",
    "        \n",
    "        # Print epoch summary\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Epoch {epoch}/{num_epochs}\")\n",
    "        print(f\"  Train Loss: {train_loss:.6f}\")\n",
    "        print(f\"  Val Loss:   {val_loss:.6f}\")\n",
    "        print(f\"  LR:         {current_lr:.2e}\")\n",
    "        \n",
    "        # Save best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            \n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'train_loss': train_loss,\n",
    "                'val_loss': val_loss,\n",
    "                'history': history\n",
    "            }, 'best_model.pth')\n",
    "            \n",
    "            print(f\"  âœ… New best model saved! (Val Loss: {val_loss:.6f})\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\"  No improvement ({patience_counter}/{max_patience})\")\n",
    "        \n",
    "        # Early stopping\n",
    "        if patience_counter >= max_patience:\n",
    "            print(f\"\\nâš ï¸ Early stopping triggered after {epoch} epochs\")\n",
    "            break\n",
    "        \n",
    "        print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    print(\"\\nâœ… Training complete!\")\n",
    "    print(f\"   Best validation loss: {best_val_loss:.6f}\")\n",
    "    \n",
    "    return history\n",
    "\n",
    "print(\"âœ… Training functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98cdb43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model hyperparameters\n",
    "model_config = {\n",
    "    'input_dim': len(train_dataset.feature_cols),\n",
    "    'hidden_dim': 256,\n",
    "    'num_heads': 8,\n",
    "    'num_layers': 6,\n",
    "    'max_players': 22,\n",
    "    'max_seq_len': 40,\n",
    "    'max_output_frames': 35,\n",
    "    'dropout': 0.1\n",
    "}\n",
    "\n",
    "print(\"ðŸ—ï¸ Initializing model...\")\n",
    "print(f\"   Config: {model_config}\")\n",
    "\n",
    "model = TrajectoryTransformer(**model_config)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"   Total parameters: {total_params:,}\")\n",
    "print(f\"   Trainable parameters: {trainable_params:,}\")\n",
    "\n",
    "# Train the model\n",
    "history = train_model(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    num_epochs=50,\n",
    "    lr=1e-4,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Save training history\n",
    "with open('training_history.pkl', 'wb') as f:\n",
    "    pickle.dump(history, f)\n",
    "\n",
    "print(\"\\nâœ… Model training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63aec357",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ensemble(num_models=5, train_loader=None, val_loader=None, device='cuda'):\n",
    "    \"\"\"\n",
    "    Train multiple models with different initializations for ensemble\n",
    "    \"\"\"\n",
    "    print(f\"\\nðŸŽ¯ Training ensemble of {num_models} models\")\n",
    "    \n",
    "    ensemble_models = []\n",
    "    ensemble_histories = []\n",
    "    \n",
    "    for i in range(num_models):\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Training Model {i+1}/{num_models}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # Set different seed for each model\n",
    "        set_seed(42 + i)\n",
    "        \n",
    "        # Initialize model\n",
    "        model = TrajectoryTransformer(**model_config)\n",
    "        \n",
    "        # Train model\n",
    "        history = train_model(\n",
    "            model=model,\n",
    "            train_loader=train_loader,\n",
    "            val_loader=val_loader,\n",
    "            num_epochs=30,  # Fewer epochs per model\n",
    "            lr=1e-4,\n",
    "            device=device\n",
    "        )\n",
    "        \n",
    "        # Load best checkpoint\n",
    "        checkpoint = torch.load('best_model.pth')\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        \n",
    "        # Save model\n",
    "        torch.save(model.state_dict(), f'ensemble_model_{i+1}.pth')\n",
    "        \n",
    "        ensemble_models.append(model)\n",
    "        ensemble_histories.append(history)\n",
    "        \n",
    "        print(f\"âœ… Model {i+1} complete!\")\n",
    "    \n",
    "    return ensemble_models, ensemble_histories\n",
    "\n",
    "\n",
    "def ensemble_predict(models, input_seq, role_ids, side_ids, num_output_frames, mask=None):\n",
    "    \"\"\"\n",
    "    Make predictions using ensemble of models\n",
    "    \"\"\"\n",
    "    all_predictions = []\n",
    "    all_uncertainties = []\n",
    "    \n",
    "    for model in models:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            pred_traj, pred_uncert = model(\n",
    "                input_seq, role_ids, side_ids, num_output_frames, mask\n",
    "            )\n",
    "            all_predictions.append(pred_traj)\n",
    "            all_uncertainties.append(pred_uncert)\n",
    "    \n",
    "    # Average predictions\n",
    "    ensemble_pred = torch.stack(all_predictions).mean(dim=0)\n",
    "    \n",
    "    # Combine uncertainties (epistemic + aleatoric)\n",
    "    mean_uncert = torch.stack(all_uncertainties).mean(dim=0)\n",
    "    pred_variance = torch.stack(all_predictions).var(dim=0)\n",
    "    ensemble_uncert = mean_uncert + pred_variance\n",
    "    \n",
    "    return ensemble_pred, ensemble_uncert\n",
    "\n",
    "\n",
    "# Train ensemble \n",
    "ensemble_models, ensemble_histories = train_ensemble(\n",
    "    num_models=5,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(\"âœ… Ensemble training functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf0d093",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PhysicsConstraints:\n",
    "    \"\"\"\n",
    "    Apply physics-based constraints to trajectory predictions\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 max_speed=12.0,        # yards/second (elite NFL speed ~23 mph)\n",
    "                 max_acceleration=8.0,   # yards/second^2\n",
    "                 dt=0.1):                # time step (10 Hz tracking)\n",
    "        self.max_speed = max_speed\n",
    "        self.max_acceleration = max_acceleration\n",
    "        self.dt = dt\n",
    "    \n",
    "    def apply_constraints(self, trajectories, initial_velocity=None):\n",
    "        \"\"\"\n",
    "        Apply physics constraints to predicted trajectories\n",
    "        \n",
    "        Args:\n",
    "            trajectories: [batch, num_players, num_frames, 2] - predicted (x, y)\n",
    "            initial_velocity: [batch, num_players, 2] - initial velocity (vx, vy)\n",
    "        \n",
    "        Returns:\n",
    "            constrained_trajectories: [batch, num_players, num_frames, 2]\n",
    "        \"\"\"\n",
    "        batch_size, num_players, num_frames, _ = trajectories.shape\n",
    "        \n",
    "        constrained = trajectories.clone()\n",
    "        \n",
    "        for b in range(batch_size):\n",
    "            for p in range(num_players):\n",
    "                # Get trajectory for this player\n",
    "                traj = constrained[b, p]  # [T, 2]\n",
    "                \n",
    "                # Initialize velocity\n",
    "                if initial_velocity is not None:\n",
    "                    velocity = initial_velocity[b, p].clone()\n",
    "                else:\n",
    "                    velocity = torch.zeros(2)\n",
    "                \n",
    "                # Apply constraints frame by frame\n",
    "                for t in range(1, num_frames):\n",
    "                    # Compute displacement\n",
    "                    displacement = traj[t] - traj[t-1]\n",
    "                    \n",
    "                    # Compute implied velocity\n",
    "                    implied_velocity = displacement / self.dt\n",
    "                    \n",
    "                    # Compute implied acceleration\n",
    "                    acceleration = (implied_velocity - velocity) / self.dt\n",
    "                    \n",
    "                    # Constrain acceleration\n",
    "                    accel_magnitude = torch.norm(acceleration)\n",
    "                    if accel_magnitude > self.max_acceleration:\n",
    "                        acceleration = acceleration * (self.max_acceleration / accel_magnitude)\n",
    "                    \n",
    "                    # Update velocity\n",
    "                    velocity = velocity + acceleration * self.dt\n",
    "                    \n",
    "                    # Constrain speed\n",
    "                    speed = torch.norm(velocity)\n",
    "                    if speed > self.max_speed:\n",
    "                        velocity = velocity * (self.max_speed / speed)\n",
    "                    \n",
    "                    # Update position\n",
    "                    constrained[b, p, t] = traj[t-1] + velocity * self.dt\n",
    "                    \n",
    "                    # Field boundaries (0-120 yards x, 0-53.3 yards y)\n",
    "                    constrained[b, p, t, 0] = torch.clamp(constrained[b, p, t, 0], 0, 120)\n",
    "                    constrained[b, p, t, 1] = torch.clamp(constrained[b, p, t, 1], 0, 53.3)\n",
    "        \n",
    "        return constrained\n",
    "    \n",
    "    def smooth_trajectory(self, trajectories, window_size=3):\n",
    "        \"\"\"\n",
    "        Apply moving average smoothing to trajectories\n",
    "        \"\"\"\n",
    "        batch_size, num_players, num_frames, coords = trajectories.shape\n",
    "        \n",
    "        smoothed = trajectories.clone()\n",
    "        \n",
    "        for b in range(batch_size):\n",
    "            for p in range(num_players):\n",
    "                for c in range(coords):\n",
    "                    # Apply 1D convolution for smoothing\n",
    "                    signal = trajectories[b, p, :, c]\n",
    "                    kernel = torch.ones(window_size) / window_size\n",
    "                    \n",
    "                    # Pad signal\n",
    "                    padded = F.pad(signal.unsqueeze(0).unsqueeze(0), \n",
    "                                   (window_size//2, window_size//2), \n",
    "                                   mode='replicate')\n",
    "                    \n",
    "                    # Convolve\n",
    "                    smoothed_signal = F.conv1d(padded, \n",
    "                                               kernel.unsqueeze(0).unsqueeze(0))\n",
    "                    \n",
    "                    smoothed[b, p, :, c] = smoothed_signal.squeeze()\n",
    "        \n",
    "        return smoothed\n",
    "\n",
    "\n",
    "# Initialize physics constraints\n",
    "physics = PhysicsConstraints(\n",
    "    max_speed=12.0,\n",
    "    max_acceleration=8.0,\n",
    "    dt=0.1\n",
    ")\n",
    "\n",
    "print(\"âœ… Physics constraints defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2b0a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContextAwarePredictor:\n",
    "    \"\"\"\n",
    "    Wrapper that incorporates play context into predictions\n",
    "    \"\"\"\n",
    "    def __init__(self, model, physics_constraints, supplementary_data):\n",
    "        self.model = model\n",
    "        self.physics = physics_constraints\n",
    "        self.supplementary = supplementary_data\n",
    "        \n",
    "        # Coverage type embeddings (learned or rule-based adjustments)\n",
    "        self.coverage_adjustments = {\n",
    "            'Cover 0': 1.0,   # Man coverage - tighter coverage\n",
    "            'Cover 1': 0.95,\n",
    "            'Cover 2': 0.9,   # Zone coverage - more space\n",
    "            'Cover 3': 0.85,\n",
    "            'Cover 4': 0.8,\n",
    "            'Cover 6': 0.85,\n",
    "            'Man': 1.0,\n",
    "            'Zone': 0.85,\n",
    "            'Prevent': 0.7    # Deep zones - lots of space\n",
    "        }\n",
    "    \n",
    "    def predict_with_context(self, batch, device='cuda'):\n",
    "        \"\"\"\n",
    "        Make context-aware predictions\n",
    "        \"\"\"\n",
    "        # Get model predictions\n",
    "        input_seq = batch['input_seq'].to(device)\n",
    "        role_ids = batch['role_ids'].to(device)\n",
    "        side_ids = batch['side_ids'].to(device)\n",
    "        num_output_frames = batch['num_output_frames']\n",
    "        mask = batch['mask'].to(device)\n",
    "        \n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            pred_traj, pred_uncert = self.model(\n",
    "                input_seq, role_ids, side_ids, num_output_frames, mask\n",
    "            )\n",
    "        \n",
    "        # Apply physics constraints\n",
    "        pred_traj = self.physics.apply_constraints(pred_traj)\n",
    "        pred_traj = self.physics.smooth_trajectory(pred_traj)\n",
    "        \n",
    "        # Apply context-based adjustments\n",
    "        for b in range(len(batch['metadata'])):\n",
    "            metadata = batch['metadata'][b]\n",
    "            game_id = metadata['game_id']\n",
    "            play_id = metadata['play_id']\n",
    "            \n",
    "            # Get play context\n",
    "            play_info = self.supplementary[\n",
    "                (self.supplementary['game_id'] == game_id) & \n",
    "                (self.supplementary['play_id'] == play_id)\n",
    "            ]\n",
    "            \n",
    "            if len(play_info) > 0:\n",
    "                coverage = play_info.iloc[0].get('team_coverage_type', 'Unknown')\n",
    "                \n",
    "                # Adjust predictions based on coverage\n",
    "                if coverage in self.coverage_adjustments:\n",
    "                    adjustment = self.coverage_adjustments[coverage]\n",
    "                    \n",
    "                    # Scale defensive player movements\n",
    "                    defense_mask = side_ids[b] == 1  # Defense\n",
    "                    pred_traj[b, defense_mask] *= adjustment\n",
    "        \n",
    "        return pred_traj, pred_uncert\n",
    "    \n",
    "    def predict_with_route_awareness(self, batch, route_patterns, device='cuda'):\n",
    "        \"\"\"\n",
    "        Incorporate route pattern knowledge\n",
    "        \"\"\"\n",
    "        # Get base predictions\n",
    "        pred_traj, pred_uncert = self.predict_with_context(batch, device)\n",
    "        \n",
    "        # Adjust receiver trajectories based on route patterns\n",
    "        for b in range(len(batch['metadata'])):\n",
    "            metadata = batch['metadata'][b]\n",
    "            \n",
    "            # Get targeted receiver\n",
    "            role_ids = batch['role_ids'][b]\n",
    "            targeted_receiver_mask = (role_ids == 1)  # Targeted Receiver\n",
    "            \n",
    "            if targeted_receiver_mask.any():\n",
    "                # Apply route-specific adjustments\n",
    "                # This could be learned or rule-based\n",
    "                pass\n",
    "        \n",
    "        return pred_traj, pred_uncert\n",
    "\n",
    "\n",
    "# Initialize context-aware predictor\n",
    "context_predictor = ContextAwarePredictor(\n",
    "    model=model,\n",
    "    physics_constraints=physics,\n",
    "    supplementary_data=train_dataset.supplementary\n",
    ")\n",
    "\n",
    "print(\"âœ… Context-aware predictor defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4bc321e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(predictions, targets, players_to_predict, num_output_frames):\n",
    "    \"\"\"\n",
    "    Compute evaluation metrics\n",
    "    \"\"\"\n",
    "    metrics = {\n",
    "        'ADE': [],  # Average Displacement Error\n",
    "        'FDE': [],  # Final Displacement Error\n",
    "        'ADE_per_frame': [],\n",
    "        'FDE_per_player': []\n",
    "    }\n",
    "    \n",
    "    batch_size = predictions.shape[0]\n",
    "    \n",
    "    for b in range(batch_size):\n",
    "        T = num_output_frames[b]\n",
    "        player_mask = players_to_predict[b]\n",
    "        \n",
    "        if player_mask.sum() == 0:\n",
    "            continue\n",
    "        \n",
    "        pred = predictions[b, player_mask, :T, :]\n",
    "        target = targets[b, player_mask, :T, :]\n",
    "        \n",
    "        # Compute displacement errors\n",
    "        displacements = torch.sqrt(((pred - target) ** 2).sum(dim=-1))\n",
    "        \n",
    "        # ADE: average over all frames and players\n",
    "        ade = displacements.mean().item()\n",
    "        metrics['ADE'].append(ade)\n",
    "        \n",
    "        # FDE: error at final frame\n",
    "        fde = displacements[:, -1].mean().item()\n",
    "        metrics['FDE'].append(fde)\n",
    "        \n",
    "        # Per-frame ADE\n",
    "        ade_per_frame = displacements.mean(dim=0).cpu().numpy()\n",
    "        metrics['ADE_per_frame'].append(ade_per_frame)\n",
    "        \n",
    "        # Per-player FDE\n",
    "        fde_per_player = displacements[:, -1].cpu().numpy()\n",
    "        metrics['FDE_per_player'].extend(fde_per_player.tolist())\n",
    "    \n",
    "    # Aggregate metrics\n",
    "    summary = {\n",
    "        'mean_ADE': np.mean(metrics['ADE']),\n",
    "        'std_ADE': np.std(metrics['ADE']),\n",
    "        'mean_FDE': np.mean(metrics['FDE']),\n",
    "        'std_FDE': np.std(metrics['FDE']),\n",
    "        'median_ADE': np.median(metrics['ADE']),\n",
    "        'median_FDE': np.median(metrics['FDE'])\n",
    "    }\n",
    "    \n",
    "    return summary, metrics\n",
    "\n",
    "\n",
    "# Evaluate on validation set\n",
    "print(\"\\nðŸ“Š Evaluating model on validation set...\")\n",
    "\n",
    "model.eval()\n",
    "all_metrics = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(val_loader, desc=\"Evaluating\"):\n",
    "        input_seq = batch['input_seq'].to(device)\n",
    "        output_seq = batch['output_seq'].to(device)\n",
    "        mask = batch['mask'].to(device)\n",
    "        role_ids = batch['role_ids'].to(device)\n",
    "        side_ids = batch['side_ids'].to(device)\n",
    "        players_to_predict = batch['players_to_predict']\n",
    "        num_output_frames = batch['num_output_frames']\n",
    "        \n",
    "        # Predict\n",
    "        pred_traj, _ = model(input_seq, role_ids, side_ids, num_output_frames, mask)\n",
    "        \n",
    "        # Apply physics constraints\n",
    "        pred_traj = physics.apply_constraints(pred_traj)\n",
    "        \n",
    "        # Compute metrics\n",
    "        summary, metrics = compute_metrics(\n",
    "            pred_traj.cpu(),\n",
    "            output_seq.cpu(),\n",
    "            players_to_predict,\n",
    "            num_output_frames\n",
    "        )\n",
    "        \n",
    "        all_metrics.append(summary)\n",
    "\n",
    "# Aggregate all metrics\n",
    "final_metrics = {\n",
    "    'ADE': np.mean([m['mean_ADE'] for m in all_metrics]),\n",
    "    'FDE': np.mean([m['mean_FDE'] for m in all_metrics]),\n",
    "    'ADE_std': np.mean([m['std_ADE'] for m in all_metrics]),\n",
    "    'FDE_std': np.mean([m['std_FDE'] for m in all_metrics])\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ðŸ“Š FINAL EVALUATION METRICS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Average Displacement Error (ADE): {final_metrics['ADE']:.4f} Â± {final_metrics['ADE_std']:.4f} yards\")\n",
    "print(f\"Final Displacement Error (FDE):   {final_metrics['FDE']:.4f} Â± {final_metrics['FDE_std']:.4f} yards\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2593577",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_prediction(input_seq, pred_traj, target_traj, metadata, player_idx=0):\n",
    "    \"\"\"\n",
    "    Visualize a single player's trajectory prediction\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(15, 8))\n",
    "    \n",
    "    # Draw field\n",
    "    ax.set_xlim(0, 120)\n",
    "    ax.set_ylim(0, 53.3)\n",
    "    ax.set_xlabel('X (yards)', fontsize=12)\n",
    "    ax.set_ylabel('Y (yards)', fontsize=12)\n",
    "    ax.set_title(f\"Trajectory Prediction - Game {metadata['game_id']}, Play {metadata['play_id']}\", \n",
    "                 fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # Draw field lines\n",
    "    for x in range(0, 121, 10):\n",
    "        ax.axvline(x, color='white', linewidth=0.5, alpha=0.3)\n",
    "    ax.axhline(53.3/2, color='white', linewidth=1, alpha=0.5)\n",
    "    \n",
    "    # Plot input trajectory\n",
    "    input_x = input_seq[:, player_idx, 0].cpu().numpy()\n",
    "    input_y = input_seq[:, player_idx, 1].cpu().numpy()\n",
    "    ax.plot(input_x, input_y, 'o-', color='blue', linewidth=2, \n",
    "            markersize=6, label='Input Trajectory', alpha=0.7)\n",
    "    \n",
    "    # Plot predicted trajectory\n",
    "    pred_x = pred_traj[player_idx, :, 0].cpu().numpy()\n",
    "    pred_y = pred_traj[player_idx, :, 1].cpu().numpy()\n",
    "    ax.plot(pred_x, pred_y, 's-', color='red', linewidth=2, \n",
    "            markersize=6, label='Predicted Trajectory', alpha=0.7)\n",
    "    \n",
    "    # Plot ground truth trajectory\n",
    "    target_x = target_traj[player_idx, :, 0].cpu().numpy()\n",
    "    target_y = target_traj[player_idx, :, 1].cpu().numpy()\n",
    "    ax.plot(target_x, target_y, '^-', color='green', linewidth=2, \n",
    "            markersize=6, label='Ground Truth', alpha=0.7)\n",
    "    \n",
    "    # Mark start and end points\n",
    "    ax.scatter(input_x[0], input_y[0], s=200, c='blue', marker='o', \n",
    "               edgecolors='black', linewidths=2, zorder=5, label='Start')\n",
    "    ax.scatter(target_x[-1], target_y[-1], s=200, c='green', marker='*', \n",
    "               edgecolors='black', linewidths=2, zorder=5, label='End (GT)')\n",
    "    ax.scatter(pred_x[-1], pred_y[-1], s=200, c='red', marker='X', \n",
    "               edgecolors='black', linewidths=2, zorder=5, label='End (Pred)')\n",
    "    \n",
    "    ax.legend(loc='upper right', fontsize=10)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_facecolor('#2d5016')  # Football field green\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "\n",
    "# Visualize some predictions\n",
    "print(\"\\nðŸŽ¨ Creating visualizations...\")\n",
    "\n",
    "# Get a batch\n",
    "sample_batch = next(iter(val_loader))\n",
    "\n",
    "with torch.no_grad():\n",
    "    input_seq = sample_batch['input_seq'].to(device)\n",
    "    output_seq = sample_batch['output_seq']\n",
    "    mask = sample_batch['mask'].to(device)\n",
    "    role_ids = sample_batch['role_ids'].to(device)\n",
    "    side_ids = sample_batch['side_ids'].to(device)\n",
    "    num_output_frames = sample_batch['num_output_frames']\n",
    "    \n",
    "    pred_traj, _ = model(input_seq, role_ids, side_ids, num_output_frames, mask)\n",
    "    pred_traj = physics.apply_constraints(pred_traj)\n",
    "\n",
    "# Visualize first sample\n",
    "fig = visualize_prediction(\n",
    "    input_seq[0].cpu(),\n",
    "    pred_traj[0].cpu(),\n",
    "    output_seq[0],\n",
    "    sample_batch['metadata'][0],\n",
    "    player_idx=0\n",
    ")\n",
    "plt.savefig('trajectory_prediction_sample.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ… Visualization saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c26024",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final model\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'model_config': model_config,\n",
    "    'feature_cols': train_dataset.feature_cols,\n",
    "    'feature_means': train_dataset.feature_means,\n",
    "    'feature_stds': train_dataset.feature_stds,\n",
    "    'final_metrics': final_metrics\n",
    "}, 'final_model_complete.pth')\n",
    "\n",
    "# Save normalization stats\n",
    "normalization_stats = {\n",
    "    'feature_means': train_dataset.feature_means,\n",
    "    'feature_stds': train_dataset.feature_stds,\n",
    "    'feature_cols': train_dataset.feature_cols\n",
    "}\n",
    "\n",
    "with open('normalization_stats.pkl', 'wb') as f:\n",
    "    pickle.dump(normalization_stats, f)\n",
    "\n",
    "# Save metrics\n",
    "with open('evaluation_metrics.json', 'w') as f:\n",
    "    json.dump(final_metrics, f, indent=2)\n",
    "\n",
    "print(\"\\nâœ… All artifacts saved!\")\n",
    "print(\"\\nSaved files:\")\n",
    "print(\"  - final_model_complete.pth\")\n",
    "print(\"  - normalization_stats.pkl\")\n",
    "print(\"  - evaluation_metrics.json\")\n",
    "print(\"  - training_history.pkl\")\n",
    "print(\"  - trajectory_prediction_sample.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
