{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7ec2225d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Core libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, CosineAnnealingWarmRestarts\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "def set_seed(seed=42):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8cc7ff06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Transformer model defined\n"
     ]
    }
   ],
   "source": [
    "class TrajectoryTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Advanced Transformer model for multi-agent trajectory prediction\n",
    "    with attention mechanisms and uncertainty estimation\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 input_dim=64,           # Feature dimension per player\n",
    "                 hidden_dim=256,         # Hidden layer size\n",
    "                 num_heads=8,            # Attention heads\n",
    "                 num_layers=6,           # Transformer layers\n",
    "                 max_players=22,         # Max players on field\n",
    "                 max_seq_len=50,         # Max input frames\n",
    "                 max_output_frames=35,   # Max frames to predict\n",
    "                 dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.max_output_frames = max_output_frames\n",
    "        \n",
    "        # Input projection\n",
    "        self.input_projection = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "        # Positional encoding for temporal information\n",
    "        self.temporal_encoding = nn.Parameter(\n",
    "            self._generate_positional_encoding(max_seq_len, hidden_dim)\n",
    "        )\n",
    "        \n",
    "        # Player role embedding (Passer, Receiver, Route Runner, Coverage, Other)\n",
    "        self.role_embedding = nn.Embedding(5, hidden_dim)\n",
    "        \n",
    "        # Side embedding (Offense/Defense)\n",
    "        self.side_embedding = nn.Embedding(2, hidden_dim)\n",
    "        \n",
    "        # Transformer encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=hidden_dim,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=hidden_dim * 4,\n",
    "            dropout=dropout,\n",
    "            activation='gelu',\n",
    "            batch_first=True,\n",
    "            norm_first=True\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            encoder_layer, \n",
    "            num_layers=num_layers\n",
    "        )\n",
    "        \n",
    "        # Cross-attention for player interactions\n",
    "        self.cross_attention = nn.MultiheadAttention(\n",
    "            embed_dim=hidden_dim,\n",
    "            num_heads=num_heads,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # Trajectory decoder with residual connections\n",
    "        self.trajectory_decoder = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(hidden_dim, hidden_dim),\n",
    "                nn.LayerNorm(hidden_dim),\n",
    "                nn.GELU(),\n",
    "                nn.Dropout(dropout)\n",
    "            ) for _ in range(3)\n",
    "        ])\n",
    "        \n",
    "        # Output head - predicts variable length trajectories\n",
    "        self.output_head = nn.Linear(hidden_dim, max_output_frames * 2)\n",
    "        \n",
    "        # Uncertainty estimation head\n",
    "        self.uncertainty_head = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_dim // 2, max_output_frames * 2)\n",
    "        )\n",
    "        \n",
    "        # Frame-specific output projection\n",
    "        self.frame_predictor = nn.Linear(hidden_dim, 2)\n",
    "        \n",
    "    def _generate_positional_encoding(self, max_len, d_model):\n",
    "        \"\"\"Generate sinusoidal positional encoding\"\"\"\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-np.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(1, max_len, d_model)\n",
    "        pe[0, :, 0::2] = torch.sin(position * div_term)\n",
    "        pe[0, :, 1::2] = torch.cos(position * div_term)\n",
    "        return pe\n",
    "        \n",
    "    def forward(self, x, role_ids, side_ids, num_output_frames, mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: [batch, seq_len, num_players, input_dim]\n",
    "            role_ids: [batch, num_players] - player role IDs (0-4)\n",
    "            side_ids: [batch, num_players] - player side IDs (0=offense, 1=defense)\n",
    "            num_output_frames: [batch] - number of frames to predict per sample\n",
    "            mask: [batch, seq_len, num_players] - attention mask\n",
    "        Returns:\n",
    "            trajectories: [batch, num_players, max_output_frames, 2]\n",
    "            uncertainties: [batch, num_players, max_output_frames, 2]\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, num_players, _ = x.shape\n",
    "        \n",
    "        # Reshape for processing: [B*P, T, D]\n",
    "        x = x.reshape(batch_size * num_players, seq_len, self.input_dim)\n",
    "        \n",
    "        # Project input\n",
    "        x = self.input_projection(x)\n",
    "        \n",
    "        # Add temporal encoding\n",
    "        x = x + self.temporal_encoding[:, :seq_len, :]\n",
    "        \n",
    "        # Add role embedding\n",
    "        role_emb = self.role_embedding(role_ids)  # [B, P, H]\n",
    "        role_emb = role_emb.reshape(batch_size * num_players, 1, self.hidden_dim)\n",
    "        x = x + role_emb\n",
    "        \n",
    "        # Add side embedding\n",
    "        side_emb = self.side_embedding(side_ids)  # [B, P, H]\n",
    "        side_emb = side_emb.reshape(batch_size * num_players, 1, self.hidden_dim)\n",
    "        x = x + side_emb\n",
    "        \n",
    "        # Apply transformer encoder\n",
    "        if mask is not None:\n",
    "            mask = mask.reshape(batch_size * num_players, seq_len)\n",
    "        encoded = self.transformer_encoder(x, src_key_padding_mask=mask)\n",
    "        \n",
    "        # Reshape back for cross-attention: [B, P, T, H]\n",
    "        encoded = encoded.reshape(batch_size, num_players, seq_len, self.hidden_dim)\n",
    "        \n",
    "        # Apply cross-attention across players (at last timestep)\n",
    "        last_encoded = encoded[:, :, -1, :]  # [B, P, H]\n",
    "        attended, _ = self.cross_attention(last_encoded, last_encoded, last_encoded)\n",
    "        \n",
    "        # Residual connection\n",
    "        last_encoded = last_encoded + attended\n",
    "        \n",
    "        # Apply decoder layers with residual connections\n",
    "        hidden = last_encoded\n",
    "        for decoder_layer in self.trajectory_decoder:\n",
    "            hidden = hidden + decoder_layer(hidden)\n",
    "        \n",
    "        # Predict trajectories\n",
    "        traj_flat = self.output_head(hidden)  # [B, P, max_T*2]\n",
    "        trajectories = traj_flat.reshape(\n",
    "            batch_size, num_players, self.max_output_frames, 2\n",
    "        )\n",
    "        \n",
    "        # Predict uncertainties (log variance)\n",
    "        uncert_flat = self.uncertainty_head(hidden)\n",
    "        uncertainties = F.softplus(uncert_flat).reshape(\n",
    "            batch_size, num_players, self.max_output_frames, 2\n",
    "        )\n",
    "        \n",
    "        return trajectories, uncertainties\n",
    "\n",
    "print(\"âœ… Transformer model defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5e1e2489",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ðŸ” DATA DIRECTORY DISCOVERY (PARQUET)\n",
      "======================================================================\n",
      "\n",
      "Current working directory:\n",
      "  c:\\Users\\jrzem\\Downloads\\NFL-Big-Data-Bowl-2026-Analytics-Challenge\\notebooks\n",
      "Resolved data_dir:\n",
      "  c:\\Users\\jrzem\\Downloads\\NFL-Big-Data-Bowl-2026-Analytics-Challenge\\notebooks\\..\\data\\processed\n",
      "\n",
      "Parquet files found in ..\\data\\processed: 18\n",
      "  - features_enhanced_w01.parquet\n",
      "  - features_enhanced_w02.parquet\n",
      "  - features_enhanced_w03.parquet\n",
      "  - features_enhanced_w04.parquet\n",
      "  - features_enhanced_w05.parquet\n",
      "  - features_enhanced_w06.parquet\n",
      "  - features_enhanced_w07.parquet\n",
      "  - features_enhanced_w08.parquet\n",
      "  - features_enhanced_w09.parquet\n",
      "  - features_enhanced_w10.parquet\n",
      "  - features_enhanced_w11.parquet\n",
      "  - features_enhanced_w12.parquet\n",
      "  - features_enhanced_w13.parquet\n",
      "  - features_enhanced_w14.parquet\n",
      "  - features_enhanced_w15.parquet\n",
      "  - features_enhanced_w16.parquet\n",
      "  - features_enhanced_w17.parquet\n",
      "  - features_enhanced_w18.parquet\n",
      "\n",
      "Available weeks parsed from filenames: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18]\n",
      "\n",
      "======================================================================\n",
      "ðŸ“Š FINAL CONFIGURATION\n",
      "======================================================================\n",
      "DATA_DIR   = ..\\data\\processed\n",
      "TRAIN_WEEKS = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "VAL_WEEKS   = [15, 16, 17, 18]\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ðŸ” DATA DIRECTORY DISCOVERY (PARQUET)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# 1. Resolve data_dir relative to this notebook\n",
    "data_dir = Path(\"data/processed\")\n",
    "if not data_dir.exists():\n",
    "    # your project root is one level up (youâ€™re in notebooks/)\n",
    "    data_dir = Path(\"../data/processed\")\n",
    "\n",
    "print(f\"\\nCurrent working directory:\\n  {os.getcwd()}\")\n",
    "print(f\"Resolved data_dir:\\n  {data_dir.absolute()}\")\n",
    "\n",
    "if not data_dir.exists():\n",
    "    raise FileNotFoundError(f\"Data directory not found at: {data_dir.absolute()}\")\n",
    "\n",
    "# 2. List parquet files\n",
    "parquet_files = sorted([f for f in os.listdir(data_dir) if f.endswith(\".parquet\")])\n",
    "print(f\"\\nParquet files found in {data_dir}: {len(parquet_files)}\")\n",
    "for f in parquet_files:\n",
    "    print(f\"  - {f}\")\n",
    "\n",
    "# 3. Extract weeks from filenames like 'features_enhanced_w01.parquet'\n",
    "available_weeks = []\n",
    "for f in parquet_files:\n",
    "    # expect pattern: features_enhanced_w01.parquet\n",
    "    if \"features_enhanced_w\" in f:\n",
    "        # split on 'w' and strip extension\n",
    "        # 'features_enhanced_w01.parquet' -> ['features_enhanced_', '01.parquet']\n",
    "        tail = f.split(\"features_enhanced_w\", 1)[1]\n",
    "        week_str = tail.replace(\".parquet\", \"\")\n",
    "        try:\n",
    "            week = int(week_str)\n",
    "            available_weeks.append(week)\n",
    "        except ValueError:\n",
    "            print(f\"âš ï¸ Could not parse week from filename: {f}\")\n",
    "\n",
    "available_weeks = sorted(set(available_weeks))\n",
    "print(\"\\nAvailable weeks parsed from filenames:\", available_weeks)\n",
    "\n",
    "if not available_weeks:\n",
    "    raise ValueError(\"No weeks parsed from parquet filenames. Check naming pattern.\")\n",
    "\n",
    "# 4. Train/val split: last 4 weeks as validation\n",
    "if len(available_weeks) > 4:\n",
    "    VAL_WEEKS = available_weeks[-4:]\n",
    "    TRAIN_WEEKS = available_weeks[:-4]\n",
    "else:\n",
    "    # fallback: simple 80/20 split\n",
    "    split_idx = max(1, int(len(available_weeks) * 0.8))\n",
    "    TRAIN_WEEKS = available_weeks[:split_idx]\n",
    "    VAL_WEEKS = available_weeks[split_idx:]\n",
    "\n",
    "DATA_DIR = str(data_dir)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ðŸ“Š FINAL CONFIGURATION\")\n",
    "print(\"=\"*70)\n",
    "print(f\"DATA_DIR   = {DATA_DIR}\")\n",
    "print(f\"TRAIN_WEEKS = {TRAIN_WEEKS}\")\n",
    "print(f\"VAL_WEEKS   = {VAL_WEEKS}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6907935a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from pathlib import Path\n",
    "\n",
    "class NFLTrajectoryDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Loads enhanced tracking features from data/processed/features_enhanced_wXX.parquet\n",
    "    Pads every play to [max_input_frames, max_players, num_features] so DataLoader can batch.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        weeks,\n",
    "        data_dir,\n",
    "        max_input_frames=40,\n",
    "        max_output_frames=35,\n",
    "        max_players=22,\n",
    "        normalize=True,\n",
    "        augment=False,\n",
    "    ):\n",
    "        self.weeks = weeks\n",
    "        self.data_dir = Path(data_dir)\n",
    "        self.max_input_frames = max_input_frames\n",
    "        self.max_output_frames = max_output_frames\n",
    "        self.max_players = max_players\n",
    "        self.normalize = normalize\n",
    "        self.augment = augment\n",
    "\n",
    "        print(f\"\\nðŸ“‚ Loading enhanced features for weeks: {weeks}\")\n",
    "        dfs = []\n",
    "        for w in weeks:\n",
    "            fname = f\"features_enhanced_w{w:02d}.parquet\"\n",
    "            fpath = self.data_dir / fname\n",
    "            if not fpath.exists():\n",
    "                raise FileNotFoundError(f\"Missing parquet file: {fpath}\")\n",
    "            print(f\"  âœ“ {fname}\")\n",
    "            dfs.append(pd.read_parquet(fpath))\n",
    "\n",
    "        self.data = pd.concat(dfs, ignore_index=True)\n",
    "        print(f\"  Total rows loaded: {len(self.data):,}\")\n",
    "\n",
    "        required = [\"game_id\", \"play_id\", \"frame_id\", \"nfl_id\", \"x\", \"y\"]\n",
    "        missing = [c for c in required if c not in self.data.columns]\n",
    "        if missing:\n",
    "            raise ValueError(f\"Missing required columns in parquet: {missing}\")\n",
    "\n",
    "        # Base features + any numeric engineered features\n",
    "        base_cols = [\"x\", \"y\", \"s\", \"a\", \"dir\", \"o\", \"absolute_yardline_number\"]\n",
    "        self.feature_cols = [c for c in base_cols if c in self.data.columns]\n",
    "\n",
    "        exclude_cols = {\n",
    "            \"game_id\", \"play_id\", \"frame_id\", \"nfl_id\",\n",
    "            \"player_to_predict\", \"player_role\", \"player_side\",\n",
    "            \"num_frames_output\"\n",
    "        }\n",
    "        for c in self.data.columns:\n",
    "            if c in exclude_cols or c in self.feature_cols:\n",
    "                continue\n",
    "            if pd.api.types.is_numeric_dtype(self.data[c]):\n",
    "                self.feature_cols.append(c)\n",
    "\n",
    "        print(f\"\\nðŸ”§ Using {len(self.feature_cols)} feature columns:\")\n",
    "        for c in self.feature_cols:\n",
    "            print(f\"  - {c}\")\n",
    "\n",
    "        # List of unique plays\n",
    "        self.plays = (\n",
    "            self.data.groupby([\"game_id\", \"play_id\"])\n",
    "            .size()\n",
    "            .reset_index()[[\"game_id\", \"play_id\"]]\n",
    "        )\n",
    "        print(f\"\\nTotal unique plays: {len(self.plays):,}\")\n",
    "\n",
    "        # normalization stats\n",
    "        if self.normalize:\n",
    "            self.feature_means = self.data[self.feature_cols].mean()\n",
    "            self.feature_stds = self.data[self.feature_cols].std().replace(0, 1.0)\n",
    "        else:\n",
    "            self.feature_means = pd.Series(0.0, index=self.feature_cols)\n",
    "            self.feature_stds = pd.Series(1.0, index=self.feature_cols)\n",
    "\n",
    "        self.role_map = {\n",
    "            \"Passer\": 0,\n",
    "            \"Targeted Receiver\": 1,\n",
    "            \"Other Route Runner\": 2,\n",
    "            \"Defensive Coverage\": 3,\n",
    "        }\n",
    "        self.side_map = {\"Offense\": 0, \"Defense\": 1}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.plays)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.plays.iloc[idx]\n",
    "        gid = row[\"game_id\"]\n",
    "        pid = row[\"play_id\"]\n",
    "\n",
    "        play_df = (\n",
    "            self.data[(self.data.game_id == gid) & (self.data.play_id == pid)]\n",
    "            .sort_values([\"frame_id\", \"nfl_id\"])\n",
    "            .copy()\n",
    "        )\n",
    "\n",
    "        frames = play_df[\"frame_id\"].unique()\n",
    "        players = play_df[\"nfl_id\"].unique()\n",
    "\n",
    "        # Original counts (for metadata only)\n",
    "        num_frames_orig = len(frames)\n",
    "        num_players_orig = len(players)\n",
    "\n",
    "        # We will ALWAYS allocate full [T, P, F]\n",
    "        T = self.max_input_frames\n",
    "        P = self.max_players\n",
    "        F = len(self.feature_cols)\n",
    "\n",
    "        X = np.zeros((T, P, F), dtype=np.float32)\n",
    "        mask = np.ones((T, P), dtype=bool)  # True = padding, False = real\n",
    "\n",
    "        role_ids = np.zeros(P, dtype=np.int64)\n",
    "        side_ids = np.zeros(P, dtype=np.int64)\n",
    "        players_to_predict = np.zeros(P, dtype=bool)\n",
    "\n",
    "        # Fill real frames/players (truncate if more than T or P)\n",
    "        for fi, f in enumerate(frames[:T]):\n",
    "            fdf = play_df[play_df.frame_id == f]\n",
    "            for pi, p in enumerate(players[:P]):\n",
    "                pdf = fdf[fdf.nfl_id == p]\n",
    "                if len(pdf) == 0:\n",
    "                    continue\n",
    "                r = pdf.iloc[0]\n",
    "\n",
    "                vals = r[self.feature_cols].fillna(0.0).values.astype(np.float32)\n",
    "                X[fi, pi, :] = vals\n",
    "                mask[fi, pi] = False\n",
    "\n",
    "                if fi == 0:\n",
    "                    role = r.get(\"player_role\", \"Defensive Coverage\")\n",
    "                    side = r.get(\"player_side\", \"Defense\")\n",
    "                    role_ids[pi] = self.role_map.get(role, 3)\n",
    "                    side_ids[pi] = self.side_map.get(side, 1)\n",
    "                    players_to_predict[pi] = bool(r.get(\"player_to_predict\", False))\n",
    "\n",
    "        # normalize entire tensor\n",
    "        if self.normalize:\n",
    "            X = (X - self.feature_means.values) / (self.feature_stds.values + 1e-8)\n",
    "\n",
    "        # simple augmentation example: flip Y / vy\n",
    "        if self.augment and np.random.rand() < 0.5:\n",
    "            if \"y\" in self.feature_cols:\n",
    "                iy = self.feature_cols.index(\"y\")\n",
    "                X[:, :, iy] = -X[:, :, iy]\n",
    "            if \"velocity_y\" in self.feature_cols:\n",
    "                ivy = self.feature_cols.index(\"velocity_y\")\n",
    "                X[:, :, ivy] = -X[:, :, ivy]\n",
    "\n",
    "        # Output: for now, placeholder zeros with max_output_frames\n",
    "        out = np.zeros((P, self.max_output_frames, 2), dtype=np.float32)\n",
    "        if \"num_frames_output\" in play_df.columns:\n",
    "            num_out = int(play_df[\"num_frames_output\"].iloc[0])\n",
    "            num_out = min(num_out, self.max_output_frames)\n",
    "        else:\n",
    "            num_out = min(self.max_output_frames, 10)\n",
    "\n",
    "        metadata = {\n",
    "            \"game_id\": int(gid),\n",
    "            \"play_id\": int(pid),\n",
    "            \"num_frames_orig\": int(num_frames_orig),\n",
    "            \"num_players_orig\": int(num_players_orig),\n",
    "            \"num_output_frames\": int(num_out),\n",
    "        }\n",
    "\n",
    "        return {\n",
    "            \"input_seq\": torch.from_numpy(X),          # [T, P, F] fixed\n",
    "            \"output_seq\": torch.from_numpy(out),       # [P, T_out, 2] fixed\n",
    "            \"mask\": torch.from_numpy(mask),            # [T, P] fixed\n",
    "            \"role_ids\": torch.from_numpy(role_ids),    # [P]\n",
    "            \"side_ids\": torch.from_numpy(side_ids),    # [P]\n",
    "            \"players_to_predict\": torch.from_numpy(players_to_predict),  # [P]\n",
    "            \"num_output_frames\": num_out,\n",
    "            \"metadata\": metadata,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "74155104",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using:\n",
      "  DATA_DIR    = ..\\data\\processed\n",
      "  TRAIN_WEEKS = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "  VAL_WEEKS   = [15, 16, 17, 18]\n",
      "\n",
      "ðŸ“‚ Loading enhanced features for weeks: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "  âœ“ features_enhanced_w01.parquet\n",
      "  âœ“ features_enhanced_w02.parquet\n",
      "  âœ“ features_enhanced_w03.parquet\n",
      "  âœ“ features_enhanced_w04.parquet\n",
      "  âœ“ features_enhanced_w05.parquet\n",
      "  âœ“ features_enhanced_w06.parquet\n",
      "  âœ“ features_enhanced_w07.parquet\n",
      "  âœ“ features_enhanced_w08.parquet\n",
      "  âœ“ features_enhanced_w09.parquet\n",
      "  âœ“ features_enhanced_w10.parquet\n",
      "  âœ“ features_enhanced_w11.parquet\n",
      "  âœ“ features_enhanced_w12.parquet\n",
      "  âœ“ features_enhanced_w13.parquet\n",
      "  âœ“ features_enhanced_w14.parquet\n",
      "  Total rows loaded: 3,749,843\n",
      "\n",
      "ðŸ”§ Using 121 feature columns:\n",
      "  - x\n",
      "  - y\n",
      "  - s\n",
      "  - a\n",
      "  - dir\n",
      "  - o\n",
      "  - absolute_yardline_number\n",
      "  - player_weight\n",
      "  - ball_land_x\n",
      "  - ball_land_y\n",
      "  - season\n",
      "  - week\n",
      "  - quarter\n",
      "  - down\n",
      "  - yards_to_go\n",
      "  - yardline_number\n",
      "  - pre_snap_home_score\n",
      "  - pre_snap_visitor_score\n",
      "  - pass_length\n",
      "  - dropback_distance\n",
      "  - defenders_in_the_box\n",
      "  - penalty_yards\n",
      "  - pre_penalty_yards_gained\n",
      "  - yards_gained\n",
      "  - expected_points\n",
      "  - expected_points_added\n",
      "  - pre_snap_home_team_win_probability\n",
      "  - pre_snap_visitor_team_win_probability\n",
      "  - home_team_win_probability_added\n",
      "  - visitor_team_win_probility_added\n",
      "  - s_smooth\n",
      "  - a_smooth\n",
      "  - jerk\n",
      "  - dir_change\n",
      "  - bearing_diff\n",
      "  - dist_to_ball_land\n",
      "  - angle_to_ball_land\n",
      "  - dist_to_qb\n",
      "  - dist_to_target\n",
      "  - dist_to_left_sideline\n",
      "  - dist_to_right_sideline\n",
      "  - dist_to_nearest_sideline\n",
      "  - dist_to_own_endzone\n",
      "  - dist_to_opp_endzone\n",
      "  - nearest_opponent_dist\n",
      "  - receiver_separation\n",
      "  - player_density_5yd\n",
      "  - team_coverage_man_zone_encoded\n",
      "  - team_coverage_type_encoded\n",
      "  - dropback_type_encoded\n",
      "  - route_of_targeted_receiver_encoded\n",
      "  - offense_formation_encoded\n",
      "  - pass_result_encoded\n",
      "  - player_position_encoded\n",
      "  - player_role_encoded\n",
      "  - player_side_encoded\n",
      "  - score_differential\n",
      "  - field_position_norm\n",
      "  - yards_to_go_norm\n",
      "  - down_distance_ratio\n",
      "  - game_clock_seconds\n",
      "  - play_action_binary\n",
      "  - two_minute_drill\n",
      "  - end_of_quarter\n",
      "  - time_remaining_norm\n",
      "  - score_diff_possession\n",
      "  - is_winning\n",
      "  - is_close_game\n",
      "  - is_blowout\n",
      "  - third_down_long\n",
      "  - fourth_down\n",
      "  - passing_down\n",
      "  - short_yardage\n",
      "  - red_zone\n",
      "  - green_zone\n",
      "  - goal_line\n",
      "  - own_territory\n",
      "  - high_pressure_situation\n",
      "  - desperation_situation\n",
      "  - conservative_situation\n",
      "  - route_completion_rate\n",
      "  - is_quick_release\n",
      "  - is_three_step\n",
      "  - is_five_step\n",
      "  - is_seven_step\n",
      "  - is_screen_pass\n",
      "  - is_short_route\n",
      "  - is_deep_route\n",
      "  - is_simple_route\n",
      "  - is_complex_route\n",
      "  - is_traditional_dropback\n",
      "  - is_rollout\n",
      "  - is_scramble\n",
      "  - is_designed_run\n",
      "  - dropback_route_mismatch\n",
      "  - dir_rad\n",
      "  - velocity_x\n",
      "  - velocity_y\n",
      "  - velocity_magnitude\n",
      "  - accel_x\n",
      "  - accel_y\n",
      "  - accel_magnitude\n",
      "  - weight_norm\n",
      "  - momentum_x\n",
      "  - momentum_y\n",
      "  - momentum_magnitude\n",
      "  - kinetic_energy\n",
      "  - velocity_change_rate\n",
      "  - direction_change_rate\n",
      "  - trajectory_curvature\n",
      "  - jerk_x\n",
      "  - jerk_y\n",
      "  - dir_to_ball_rad\n",
      "  - velocity_towards_ball\n",
      "  - movement_efficiency\n",
      "  - is_stationary\n",
      "  - is_sprinting\n",
      "  - is_max_speed\n",
      "  - is_accelerating\n",
      "  - is_decelerating\n",
      "  - is_constant_speed\n",
      "\n",
      "Total unique plays: 10,862\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 85.8 MiB for an array with shape (3, 3749843) and data type int64",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mMemoryError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[46]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  TRAIN_WEEKS = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mTRAIN_WEEKS\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  VAL_WEEKS   = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mVAL_WEEKS\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m train_dataset = \u001b[43mNFLTrajectoryDataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweeks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mTRAIN_WEEKS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mDATA_DIR\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnormalize\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43maugment\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m val_dataset = NFLTrajectoryDataset(\n\u001b[32m     16\u001b[39m     weeks=VAL_WEEKS,\n\u001b[32m     17\u001b[39m     data_dir=DATA_DIR,\n\u001b[32m     18\u001b[39m     normalize=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m     19\u001b[39m     augment=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m     20\u001b[39m )\n\u001b[32m     22\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mTrain plays: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(train_dataset)\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[45]\u001b[39m\u001b[32m, line 77\u001b[39m, in \u001b[36mNFLTrajectoryDataset.__init__\u001b[39m\u001b[34m(self, weeks, data_dir, max_input_frames, max_output_frames, max_players, normalize, augment)\u001b[39m\n\u001b[32m     75\u001b[39m \u001b[38;5;66;03m# normalization stats\u001b[39;00m\n\u001b[32m     76\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.normalize:\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m     \u001b[38;5;28mself\u001b[39m.feature_means = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfeature_cols\u001b[49m\u001b[43m]\u001b[49m.mean()\n\u001b[32m     78\u001b[39m     \u001b[38;5;28mself\u001b[39m.feature_stds = \u001b[38;5;28mself\u001b[39m.data[\u001b[38;5;28mself\u001b[39m.feature_cols].std().replace(\u001b[32m0\u001b[39m, \u001b[32m1.0\u001b[39m)\n\u001b[32m     79\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jrzem\\Downloads\\.venv\\Lib\\site-packages\\pandas\\core\\frame.py:4128\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4125\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(indexer, \u001b[38;5;28mslice\u001b[39m):\n\u001b[32m   4126\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._slice(indexer, axis=\u001b[32m1\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m4128\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_take_with_is_copy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   4130\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_single_key:\n\u001b[32m   4131\u001b[39m     \u001b[38;5;66;03m# What does looking for a single key in a non-unique index return?\u001b[39;00m\n\u001b[32m   4132\u001b[39m     \u001b[38;5;66;03m# The behavior is inconsistent. It returns a Series, except when\u001b[39;00m\n\u001b[32m   4133\u001b[39m     \u001b[38;5;66;03m# - the key itself is repeated (test on data.shape, #9519), or\u001b[39;00m\n\u001b[32m   4134\u001b[39m     \u001b[38;5;66;03m# - we have a MultiIndex on columns (test on self.columns, #21309)\u001b[39;00m\n\u001b[32m   4135\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m data.shape[\u001b[32m1\u001b[39m] == \u001b[32m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m.columns, MultiIndex):\n\u001b[32m   4136\u001b[39m         \u001b[38;5;66;03m# GH#26490 using data[key] can cause RecursionError\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jrzem\\Downloads\\.venv\\Lib\\site-packages\\pandas\\core\\generic.py:4175\u001b[39m, in \u001b[36mNDFrame._take_with_is_copy\u001b[39m\u001b[34m(self, indices, axis)\u001b[39m\n\u001b[32m   4164\u001b[39m \u001b[38;5;129m@final\u001b[39m\n\u001b[32m   4165\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_take_with_is_copy\u001b[39m(\u001b[38;5;28mself\u001b[39m, indices, axis: Axis = \u001b[32m0\u001b[39m) -> Self:\n\u001b[32m   4166\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   4167\u001b[39m \u001b[33;03m    Internal version of the `take` method that sets the `_is_copy`\u001b[39;00m\n\u001b[32m   4168\u001b[39m \u001b[33;03m    attribute to keep track of the parent dataframe (using in indexing\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   4173\u001b[39m \u001b[33;03m    See the docstring of `take` for full explanation of the parameters.\u001b[39;00m\n\u001b[32m   4174\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m4175\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtake\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindices\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4176\u001b[39m     \u001b[38;5;66;03m# Maybe set copy if we didn't actually change the index.\u001b[39;00m\n\u001b[32m   4177\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ndim == \u001b[32m2\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m result._get_axis(axis).equals(\u001b[38;5;28mself\u001b[39m._get_axis(axis)):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jrzem\\Downloads\\.venv\\Lib\\site-packages\\pandas\\core\\generic.py:4155\u001b[39m, in \u001b[36mNDFrame.take\u001b[39m\u001b[34m(self, indices, axis, **kwargs)\u001b[39m\n\u001b[32m   4150\u001b[39m     \u001b[38;5;66;03m# We can get here with a slice via DataFrame.__getitem__\u001b[39;00m\n\u001b[32m   4151\u001b[39m     indices = np.arange(\n\u001b[32m   4152\u001b[39m         indices.start, indices.stop, indices.step, dtype=np.intp\n\u001b[32m   4153\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m4155\u001b[39m new_data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_mgr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtake\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4156\u001b[39m \u001b[43m    \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4157\u001b[39m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_block_manager_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4158\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverify\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   4159\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4160\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._constructor_from_mgr(new_data, axes=new_data.axes).__finalize__(\n\u001b[32m   4161\u001b[39m     \u001b[38;5;28mself\u001b[39m, method=\u001b[33m\"\u001b[39m\u001b[33mtake\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   4162\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jrzem\\Downloads\\.venv\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:913\u001b[39m, in \u001b[36mBaseBlockManager.take\u001b[39m\u001b[34m(self, indexer, axis, verify)\u001b[39m\n\u001b[32m    910\u001b[39m indexer = maybe_convert_indices(indexer, n, verify=verify)\n\u001b[32m    912\u001b[39m new_labels = \u001b[38;5;28mself\u001b[39m.axes[axis].take(indexer)\n\u001b[32m--> \u001b[39m\u001b[32m913\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mreindex_indexer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    914\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnew_axis\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnew_labels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    915\u001b[39m \u001b[43m    \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    916\u001b[39m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    917\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_dups\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    918\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    919\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jrzem\\Downloads\\.venv\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:699\u001b[39m, in \u001b[36mBaseBlockManager.reindex_indexer\u001b[39m\u001b[34m(self, new_axis, indexer, axis, fill_value, allow_dups, copy, only_slice, use_na_proxy)\u001b[39m\n\u001b[32m    696\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mRequested axis not found in manager\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    698\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m axis == \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m699\u001b[39m     new_blocks = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_slice_take_blocks_ax0\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    700\u001b[39m \u001b[43m        \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    701\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    702\u001b[39m \u001b[43m        \u001b[49m\u001b[43monly_slice\u001b[49m\u001b[43m=\u001b[49m\u001b[43monly_slice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    703\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_na_proxy\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_na_proxy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    704\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    705\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    706\u001b[39m     new_blocks = [\n\u001b[32m    707\u001b[39m         blk.take_nd(\n\u001b[32m    708\u001b[39m             indexer,\n\u001b[32m   (...)\u001b[39m\u001b[32m    714\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m blk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.blocks\n\u001b[32m    715\u001b[39m     ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jrzem\\Downloads\\.venv\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:862\u001b[39m, in \u001b[36mBaseBlockManager._slice_take_blocks_ax0\u001b[39m\u001b[34m(self, slice_or_indexer, fill_value, only_slice, use_na_proxy, ref_inplace_op)\u001b[39m\n\u001b[32m    860\u001b[39m                     blocks.append(nb)\n\u001b[32m    861\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m862\u001b[39m                 nb = \u001b[43mblk\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtake_nd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtaker\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_mgr_locs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmgr_locs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    863\u001b[39m                 blocks.append(nb)\n\u001b[32m    865\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m blocks\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jrzem\\Downloads\\.venv\\Lib\\site-packages\\pandas\\core\\internals\\blocks.py:1373\u001b[39m, in \u001b[36mBlock.take_nd\u001b[39m\u001b[34m(self, indexer, axis, new_mgr_locs, fill_value)\u001b[39m\n\u001b[32m   1370\u001b[39m     allow_fill = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   1372\u001b[39m \u001b[38;5;66;03m# Note: algos.take_nd has upcast logic similar to coerce_to_target_dtype\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1373\u001b[39m new_values = \u001b[43malgos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtake_nd\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1374\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_fill\u001b[49m\u001b[43m=\u001b[49m\u001b[43mallow_fill\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfill_value\u001b[49m\n\u001b[32m   1375\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1377\u001b[39m \u001b[38;5;66;03m# Called from three places in managers, all of which satisfy\u001b[39;00m\n\u001b[32m   1378\u001b[39m \u001b[38;5;66;03m#  these assertions\u001b[39;00m\n\u001b[32m   1379\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ExtensionBlock):\n\u001b[32m   1380\u001b[39m     \u001b[38;5;66;03m# NB: in this case, the 'axis' kwarg will be ignored in the\u001b[39;00m\n\u001b[32m   1381\u001b[39m     \u001b[38;5;66;03m#  algos.take_nd call above.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jrzem\\Downloads\\.venv\\Lib\\site-packages\\pandas\\core\\array_algos\\take.py:117\u001b[39m, in \u001b[36mtake_nd\u001b[39m\u001b[34m(arr, indexer, axis, fill_value, allow_fill)\u001b[39m\n\u001b[32m    114\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m arr.take(indexer, fill_value=fill_value, allow_fill=allow_fill)\n\u001b[32m    116\u001b[39m arr = np.asarray(arr)\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_take_nd_ndarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_fill\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jrzem\\Downloads\\.venv\\Lib\\site-packages\\pandas\\core\\array_algos\\take.py:157\u001b[39m, in \u001b[36m_take_nd_ndarray\u001b[39m\u001b[34m(arr, indexer, axis, fill_value, allow_fill)\u001b[39m\n\u001b[32m    155\u001b[39m     out = np.empty(out_shape, dtype=dtype, order=\u001b[33m\"\u001b[39m\u001b[33mF\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    156\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m157\u001b[39m     out = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mempty\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    159\u001b[39m func = _get_take_nd_function(\n\u001b[32m    160\u001b[39m     arr.ndim, arr.dtype, out.dtype, axis=axis, mask_info=mask_info\n\u001b[32m    161\u001b[39m )\n\u001b[32m    162\u001b[39m func(arr, indexer, out, fill_value)\n",
      "\u001b[31mMemoryError\u001b[39m: Unable to allocate 85.8 MiB for an array with shape (3, 3749843) and data type int64"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "print(\"Using:\")\n",
    "print(f\"  DATA_DIR    = {DATA_DIR}\")\n",
    "print(f\"  TRAIN_WEEKS = {TRAIN_WEEKS}\")\n",
    "print(f\"  VAL_WEEKS   = {VAL_WEEKS}\")\n",
    "\n",
    "train_dataset = NFLTrajectoryDataset(\n",
    "    weeks=TRAIN_WEEKS,\n",
    "    data_dir=DATA_DIR,\n",
    "    normalize=True,\n",
    "    augment=True,\n",
    ")\n",
    "\n",
    "val_dataset = NFLTrajectoryDataset(\n",
    "    weeks=VAL_WEEKS,\n",
    "    data_dir=DATA_DIR,\n",
    "    normalize=True,\n",
    "    augment=False,\n",
    ")\n",
    "\n",
    "print(f\"\\nTrain plays: {len(train_dataset):,}\")\n",
    "print(f\"Val plays:   {len(val_dataset):,}\")\n",
    "\n",
    "batch_size = 8\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# quick sanity check\n",
    "batch = next(iter(train_loader))\n",
    "print(\"\\nBatch shapes:\")\n",
    "print(\"  input_seq:\", batch[\"input_seq\"].shape)      # [B, T, P, F]\n",
    "print(\"  output_seq:\", batch[\"output_seq\"].shape)    # [B, P, T_out, 2]\n",
    "print(\"  mask:\", batch[\"mask\"].shape)                # [B, T, P]\n",
    "print(\"  role_ids:\", batch[\"role_ids\"].shape)        # [B, P]\n",
    "print(\"  side_ids:\", batch[\"side_ids\"].shape)        # [B, P]\n",
    "print(\"  players_to_predict:\", batch[\"players_to_predict\"].shape)  # [B, P]\n",
    "print(\"  num_output_frames (first):\", batch[\"num_output_frames\"])\n",
    "print(\"  metadata[0]:\", batch[\"metadata\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234a5313",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrajectoryLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Custom loss function for trajectory prediction\n",
    "    Combines MSE with uncertainty weighting\n",
    "    \"\"\"\n",
    "    def __init__(self, uncertainty_weight=0.1):\n",
    "        super().__init__()\n",
    "        self.uncertainty_weight = uncertainty_weight\n",
    "    \n",
    "    def forward(self, pred_traj, pred_uncert, target_traj, players_to_predict, num_output_frames):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            pred_traj: [B, P, T, 2] - predicted trajectories\n",
    "            pred_uncert: [B, P, T, 2] - predicted uncertainties\n",
    "            target_traj: [B, P, T, 2] - ground truth trajectories\n",
    "            players_to_predict: [B, P] - mask for which players to evaluate\n",
    "            num_output_frames: [B] - actual number of frames per sample\n",
    "        \"\"\"\n",
    "        batch_size = pred_traj.shape[0]\n",
    "        total_loss = 0.0\n",
    "        \n",
    "        for b in range(batch_size):\n",
    "            # Get actual frames for this sample\n",
    "            T = num_output_frames[b]\n",
    "            \n",
    "            # Get players to predict\n",
    "            player_mask = players_to_predict[b]\n",
    "            \n",
    "            if player_mask.sum() == 0:\n",
    "                continue\n",
    "            \n",
    "            # Extract relevant predictions and targets\n",
    "            pred = pred_traj[b, player_mask, :T, :]\n",
    "            uncert = pred_uncert[b, player_mask, :T, :]\n",
    "            target = target_traj[b, player_mask, :T, :]\n",
    "            \n",
    "            # Compute MSE loss\n",
    "            mse_loss = F.mse_loss(pred, target)\n",
    "            \n",
    "            # Uncertainty-weighted loss (negative log likelihood)\n",
    "            # loss = 0.5 * (error^2 / variance + log(variance))\n",
    "            error_sq = (pred - target) ** 2\n",
    "            nll_loss = 0.5 * (error_sq / (uncert + 1e-6) + torch.log(uncert + 1e-6))\n",
    "            nll_loss = nll_loss.mean()\n",
    "            \n",
    "            # Combine losses\n",
    "            total_loss += mse_loss + self.uncertainty_weight * nll_loss\n",
    "        \n",
    "        return total_loss / batch_size\n",
    "\n",
    "print(\"âœ… Loss function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8c78cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, optimizer, criterion, device, epoch):\n",
    "    \"\"\"Train for one epoch\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    num_batches = 0\n",
    "    \n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch} [Train]\")\n",
    "    \n",
    "    for batch in pbar:\n",
    "        # Move data to device\n",
    "        input_seq = batch['input_seq'].to(device)\n",
    "        output_seq = batch['output_seq'].to(device)\n",
    "        mask = batch['mask'].to(device)\n",
    "        role_ids = batch['role_ids'].to(device)\n",
    "        side_ids = batch['side_ids'].to(device)\n",
    "        players_to_predict = batch['players_to_predict'].to(device)\n",
    "        num_output_frames = batch['num_output_frames']\n",
    "        \n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        pred_traj, pred_uncert = model(\n",
    "            input_seq, \n",
    "            role_ids, \n",
    "            side_ids, \n",
    "            num_output_frames,\n",
    "            mask=mask\n",
    "        )\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = criterion(\n",
    "            pred_traj, \n",
    "            pred_uncert, \n",
    "            output_seq, \n",
    "            players_to_predict,\n",
    "            num_output_frames\n",
    "        )\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Track loss\n",
    "        total_loss += loss.item()\n",
    "        num_batches += 1\n",
    "        \n",
    "        # Update progress bar\n",
    "        pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "    \n",
    "    return total_loss / num_batches\n",
    "\n",
    "\n",
    "def validate_epoch(model, val_loader, criterion, device, epoch):\n",
    "    \"\"\"Validate for one epoch\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    num_batches = 0\n",
    "    \n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "    all_metadata = []\n",
    "    \n",
    "    pbar = tqdm(val_loader, desc=f\"Epoch {epoch} [Val]\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in pbar:\n",
    "            # Move data to device\n",
    "            input_seq = batch['input_seq'].to(device)\n",
    "            output_seq = batch['output_seq'].to(device)\n",
    "            mask = batch['mask'].to(device)\n",
    "            role_ids = batch['role_ids'].to(device)\n",
    "            side_ids = batch['side_ids'].to(device)\n",
    "            players_to_predict = batch['players_to_predict'].to(device)\n",
    "            num_output_frames = batch['num_output_frames']\n",
    "            \n",
    "            # Forward pass\n",
    "            pred_traj, pred_uncert = model(\n",
    "                input_seq, \n",
    "                role_ids, \n",
    "                side_ids, \n",
    "                num_output_frames,\n",
    "                mask=mask\n",
    "            )\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = criterion(\n",
    "                pred_traj, \n",
    "                pred_uncert, \n",
    "                output_seq, \n",
    "                players_to_predict,\n",
    "                num_output_frames\n",
    "            )\n",
    "            \n",
    "            # Track loss\n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "            \n",
    "            # Store predictions for analysis\n",
    "            all_predictions.append(pred_traj.cpu())\n",
    "            all_targets.append(output_seq.cpu())\n",
    "            all_metadata.append(batch['metadata'])\n",
    "            \n",
    "            # Update progress bar\n",
    "            pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "    \n",
    "    return total_loss / num_batches, all_predictions, all_targets, all_metadata\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, num_epochs=50, lr=1e-4, device='cuda'):\n",
    "    \"\"\"\n",
    "    Complete training loop with validation\n",
    "    \"\"\"\n",
    "    print(f\"\\nðŸš€ Starting training for {num_epochs} epochs\")\n",
    "    print(f\"   Device: {device}\")\n",
    "    print(f\"   Learning rate: {lr}\")\n",
    "    \n",
    "    # Move model to device\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer = AdamW(\n",
    "        model.parameters(), \n",
    "        lr=lr, \n",
    "        weight_decay=1e-5,\n",
    "        betas=(0.9, 0.999)\n",
    "    )\n",
    "    \n",
    "    # Learning rate scheduler\n",
    "    scheduler = ReduceLROnPlateau(\n",
    "        optimizer, \n",
    "        mode='min', \n",
    "        factor=0.5, \n",
    "        patience=5, \n",
    "        verbose=True,\n",
    "        min_lr=1e-7\n",
    "    )\n",
    "    \n",
    "    # Loss function\n",
    "    criterion = TrajectoryLoss(uncertainty_weight=0.1)\n",
    "    \n",
    "    # Training history\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'val_loss': [],\n",
    "        'learning_rates': []\n",
    "    }\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    max_patience = 10\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        # Train\n",
    "        train_loss = train_epoch(model, train_loader, optimizer, criterion, device, epoch)\n",
    "        \n",
    "        # Validate\n",
    "        val_loss, val_preds, val_targets, val_metadata = validate_epoch(\n",
    "            model, val_loader, criterion, device, epoch\n",
    "        )\n",
    "        \n",
    "        # Update scheduler\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        # Get current learning rate\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        \n",
    "        # Save history\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['learning_rates'].append(current_lr)\n",
    "        \n",
    "        # Print epoch summary\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Epoch {epoch}/{num_epochs}\")\n",
    "        print(f\"  Train Loss: {train_loss:.6f}\")\n",
    "        print(f\"  Val Loss:   {val_loss:.6f}\")\n",
    "        print(f\"  LR:         {current_lr:.2e}\")\n",
    "        \n",
    "        # Save best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            \n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'train_loss': train_loss,\n",
    "                'val_loss': val_loss,\n",
    "                'history': history\n",
    "            }, 'best_model.pth')\n",
    "            \n",
    "            print(f\"  âœ… New best model saved! (Val Loss: {val_loss:.6f})\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\"  No improvement ({patience_counter}/{max_patience})\")\n",
    "        \n",
    "        # Early stopping\n",
    "        if patience_counter >= max_patience:\n",
    "            print(f\"\\nâš ï¸ Early stopping triggered after {epoch} epochs\")\n",
    "            break\n",
    "        \n",
    "        print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    print(\"\\nâœ… Training complete!\")\n",
    "    print(f\"   Best validation loss: {best_val_loss:.6f}\")\n",
    "    \n",
    "    return history\n",
    "\n",
    "print(\"âœ… Training functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98cdb43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model hyperparameters\n",
    "model_config = {\n",
    "    'input_dim': len(train_dataset.feature_cols),\n",
    "    'hidden_dim': 256,\n",
    "    'num_heads': 8,\n",
    "    'num_layers': 6,\n",
    "    'max_players': 22,\n",
    "    'max_seq_len': 40,\n",
    "    'max_output_frames': 35,\n",
    "    'dropout': 0.1\n",
    "}\n",
    "\n",
    "print(\"ðŸ—ï¸ Initializing model...\")\n",
    "print(f\"   Config: {model_config}\")\n",
    "\n",
    "model = TrajectoryTransformer(**model_config)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"   Total parameters: {total_params:,}\")\n",
    "print(f\"   Trainable parameters: {trainable_params:,}\")\n",
    "\n",
    "# Train the model\n",
    "history = train_model(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    num_epochs=50,\n",
    "    lr=1e-4,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Save training history\n",
    "with open('training_history.pkl', 'wb') as f:\n",
    "    pickle.dump(history, f)\n",
    "\n",
    "print(\"\\nâœ… Model training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63aec357",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ensemble(num_models=5, train_loader=None, val_loader=None, device='cuda'):\n",
    "    \"\"\"\n",
    "    Train multiple models with different initializations for ensemble\n",
    "    \"\"\"\n",
    "    print(f\"\\nðŸŽ¯ Training ensemble of {num_models} models\")\n",
    "    \n",
    "    ensemble_models = []\n",
    "    ensemble_histories = []\n",
    "    \n",
    "    for i in range(num_models):\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Training Model {i+1}/{num_models}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # Set different seed for each model\n",
    "        set_seed(42 + i)\n",
    "        \n",
    "        # Initialize model\n",
    "        model = TrajectoryTransformer(**model_config)\n",
    "        \n",
    "        # Train model\n",
    "        history = train_model(\n",
    "            model=model,\n",
    "            train_loader=train_loader,\n",
    "            val_loader=val_loader,\n",
    "            num_epochs=30,  # Fewer epochs per model\n",
    "            lr=1e-4,\n",
    "            device=device\n",
    "        )\n",
    "        \n",
    "        # Load best checkpoint\n",
    "        checkpoint = torch.load('best_model.pth')\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        \n",
    "        # Save model\n",
    "        torch.save(model.state_dict(), f'ensemble_model_{i+1}.pth')\n",
    "        \n",
    "        ensemble_models.append(model)\n",
    "        ensemble_histories.append(history)\n",
    "        \n",
    "        print(f\"âœ… Model {i+1} complete!\")\n",
    "    \n",
    "    return ensemble_models, ensemble_histories\n",
    "\n",
    "\n",
    "def ensemble_predict(models, input_seq, role_ids, side_ids, num_output_frames, mask=None):\n",
    "    \"\"\"\n",
    "    Make predictions using ensemble of models\n",
    "    \"\"\"\n",
    "    all_predictions = []\n",
    "    all_uncertainties = []\n",
    "    \n",
    "    for model in models:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            pred_traj, pred_uncert = model(\n",
    "                input_seq, role_ids, side_ids, num_output_frames, mask\n",
    "            )\n",
    "            all_predictions.append(pred_traj)\n",
    "            all_uncertainties.append(pred_uncert)\n",
    "    \n",
    "    # Average predictions\n",
    "    ensemble_pred = torch.stack(all_predictions).mean(dim=0)\n",
    "    \n",
    "    # Combine uncertainties (epistemic + aleatoric)\n",
    "    mean_uncert = torch.stack(all_uncertainties).mean(dim=0)\n",
    "    pred_variance = torch.stack(all_predictions).var(dim=0)\n",
    "    ensemble_uncert = mean_uncert + pred_variance\n",
    "    \n",
    "    return ensemble_pred, ensemble_uncert\n",
    "\n",
    "\n",
    "# Train ensemble \n",
    "ensemble_models, ensemble_histories = train_ensemble(\n",
    "    num_models=5,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(\"âœ… Ensemble training functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf0d093",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PhysicsConstraints:\n",
    "    \"\"\"\n",
    "    Apply physics-based constraints to trajectory predictions\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 max_speed=12.0,        # yards/second (elite NFL speed ~23 mph)\n",
    "                 max_acceleration=8.0,   # yards/second^2\n",
    "                 dt=0.1):                # time step (10 Hz tracking)\n",
    "        self.max_speed = max_speed\n",
    "        self.max_acceleration = max_acceleration\n",
    "        self.dt = dt\n",
    "    \n",
    "    def apply_constraints(self, trajectories, initial_velocity=None):\n",
    "        \"\"\"\n",
    "        Apply physics constraints to predicted trajectories\n",
    "        \n",
    "        Args:\n",
    "            trajectories: [batch, num_players, num_frames, 2] - predicted (x, y)\n",
    "            initial_velocity: [batch, num_players, 2] - initial velocity (vx, vy)\n",
    "        \n",
    "        Returns:\n",
    "            constrained_trajectories: [batch, num_players, num_frames, 2]\n",
    "        \"\"\"\n",
    "        batch_size, num_players, num_frames, _ = trajectories.shape\n",
    "        \n",
    "        constrained = trajectories.clone()\n",
    "        \n",
    "        for b in range(batch_size):\n",
    "            for p in range(num_players):\n",
    "                # Get trajectory for this player\n",
    "                traj = constrained[b, p]  # [T, 2]\n",
    "                \n",
    "                # Initialize velocity\n",
    "                if initial_velocity is not None:\n",
    "                    velocity = initial_velocity[b, p].clone()\n",
    "                else:\n",
    "                    velocity = torch.zeros(2)\n",
    "                \n",
    "                # Apply constraints frame by frame\n",
    "                for t in range(1, num_frames):\n",
    "                    # Compute displacement\n",
    "                    displacement = traj[t] - traj[t-1]\n",
    "                    \n",
    "                    # Compute implied velocity\n",
    "                    implied_velocity = displacement / self.dt\n",
    "                    \n",
    "                    # Compute implied acceleration\n",
    "                    acceleration = (implied_velocity - velocity) / self.dt\n",
    "                    \n",
    "                    # Constrain acceleration\n",
    "                    accel_magnitude = torch.norm(acceleration)\n",
    "                    if accel_magnitude > self.max_acceleration:\n",
    "                        acceleration = acceleration * (self.max_acceleration / accel_magnitude)\n",
    "                    \n",
    "                    # Update velocity\n",
    "                    velocity = velocity + acceleration * self.dt\n",
    "                    \n",
    "                    # Constrain speed\n",
    "                    speed = torch.norm(velocity)\n",
    "                    if speed > self.max_speed:\n",
    "                        velocity = velocity * (self.max_speed / speed)\n",
    "                    \n",
    "                    # Update position\n",
    "                    constrained[b, p, t] = traj[t-1] + velocity * self.dt\n",
    "                    \n",
    "                    # Field boundaries (0-120 yards x, 0-53.3 yards y)\n",
    "                    constrained[b, p, t, 0] = torch.clamp(constrained[b, p, t, 0], 0, 120)\n",
    "                    constrained[b, p, t, 1] = torch.clamp(constrained[b, p, t, 1], 0, 53.3)\n",
    "        \n",
    "        return constrained\n",
    "    \n",
    "    def smooth_trajectory(self, trajectories, window_size=3):\n",
    "        \"\"\"\n",
    "        Apply moving average smoothing to trajectories\n",
    "        \"\"\"\n",
    "        batch_size, num_players, num_frames, coords = trajectories.shape\n",
    "        \n",
    "        smoothed = trajectories.clone()\n",
    "        \n",
    "        for b in range(batch_size):\n",
    "            for p in range(num_players):\n",
    "                for c in range(coords):\n",
    "                    # Apply 1D convolution for smoothing\n",
    "                    signal = trajectories[b, p, :, c]\n",
    "                    kernel = torch.ones(window_size) / window_size\n",
    "                    \n",
    "                    # Pad signal\n",
    "                    padded = F.pad(signal.unsqueeze(0).unsqueeze(0), \n",
    "                                   (window_size//2, window_size//2), \n",
    "                                   mode='replicate')\n",
    "                    \n",
    "                    # Convolve\n",
    "                    smoothed_signal = F.conv1d(padded, \n",
    "                                               kernel.unsqueeze(0).unsqueeze(0))\n",
    "                    \n",
    "                    smoothed[b, p, :, c] = smoothed_signal.squeeze()\n",
    "        \n",
    "        return smoothed\n",
    "\n",
    "\n",
    "# Initialize physics constraints\n",
    "physics = PhysicsConstraints(\n",
    "    max_speed=12.0,\n",
    "    max_acceleration=8.0,\n",
    "    dt=0.1\n",
    ")\n",
    "\n",
    "print(\"âœ… Physics constraints defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2b0a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContextAwarePredictor:\n",
    "    \"\"\"\n",
    "    Wrapper that incorporates play context into predictions\n",
    "    \"\"\"\n",
    "    def __init__(self, model, physics_constraints, supplementary_data):\n",
    "        self.model = model\n",
    "        self.physics = physics_constraints\n",
    "        self.supplementary = supplementary_data\n",
    "        \n",
    "        # Coverage type embeddings (learned or rule-based adjustments)\n",
    "        self.coverage_adjustments = {\n",
    "            'Cover 0': 1.0,   # Man coverage - tighter coverage\n",
    "            'Cover 1': 0.95,\n",
    "            'Cover 2': 0.9,   # Zone coverage - more space\n",
    "            'Cover 3': 0.85,\n",
    "            'Cover 4': 0.8,\n",
    "            'Cover 6': 0.85,\n",
    "            'Man': 1.0,\n",
    "            'Zone': 0.85,\n",
    "            'Prevent': 0.7    # Deep zones - lots of space\n",
    "        }\n",
    "    \n",
    "    def predict_with_context(self, batch, device='cuda'):\n",
    "        \"\"\"\n",
    "        Make context-aware predictions\n",
    "        \"\"\"\n",
    "        # Get model predictions\n",
    "        input_seq = batch['input_seq'].to(device)\n",
    "        role_ids = batch['role_ids'].to(device)\n",
    "        side_ids = batch['side_ids'].to(device)\n",
    "        num_output_frames = batch['num_output_frames']\n",
    "        mask = batch['mask'].to(device)\n",
    "        \n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            pred_traj, pred_uncert = self.model(\n",
    "                input_seq, role_ids, side_ids, num_output_frames, mask\n",
    "            )\n",
    "        \n",
    "        # Apply physics constraints\n",
    "        pred_traj = self.physics.apply_constraints(pred_traj)\n",
    "        pred_traj = self.physics.smooth_trajectory(pred_traj)\n",
    "        \n",
    "        # Apply context-based adjustments\n",
    "        for b in range(len(batch['metadata'])):\n",
    "            metadata = batch['metadata'][b]\n",
    "            game_id = metadata['game_id']\n",
    "            play_id = metadata['play_id']\n",
    "            \n",
    "            # Get play context\n",
    "            play_info = self.supplementary[\n",
    "                (self.supplementary['game_id'] == game_id) & \n",
    "                (self.supplementary['play_id'] == play_id)\n",
    "            ]\n",
    "            \n",
    "            if len(play_info) > 0:\n",
    "                coverage = play_info.iloc[0].get('team_coverage_type', 'Unknown')\n",
    "                \n",
    "                # Adjust predictions based on coverage\n",
    "                if coverage in self.coverage_adjustments:\n",
    "                    adjustment = self.coverage_adjustments[coverage]\n",
    "                    \n",
    "                    # Scale defensive player movements\n",
    "                    defense_mask = side_ids[b] == 1  # Defense\n",
    "                    pred_traj[b, defense_mask] *= adjustment\n",
    "        \n",
    "        return pred_traj, pred_uncert\n",
    "    \n",
    "    def predict_with_route_awareness(self, batch, route_patterns, device='cuda'):\n",
    "        \"\"\"\n",
    "        Incorporate route pattern knowledge\n",
    "        \"\"\"\n",
    "        # Get base predictions\n",
    "        pred_traj, pred_uncert = self.predict_with_context(batch, device)\n",
    "        \n",
    "        # Adjust receiver trajectories based on route patterns\n",
    "        for b in range(len(batch['metadata'])):\n",
    "            metadata = batch['metadata'][b]\n",
    "            \n",
    "            # Get targeted receiver\n",
    "            role_ids = batch['role_ids'][b]\n",
    "            targeted_receiver_mask = (role_ids == 1)  # Targeted Receiver\n",
    "            \n",
    "            if targeted_receiver_mask.any():\n",
    "                # Apply route-specific adjustments\n",
    "                # This could be learned or rule-based\n",
    "                pass\n",
    "        \n",
    "        return pred_traj, pred_uncert\n",
    "\n",
    "\n",
    "# Initialize context-aware predictor\n",
    "context_predictor = ContextAwarePredictor(\n",
    "    model=model,\n",
    "    physics_constraints=physics,\n",
    "    supplementary_data=train_dataset.supplementary\n",
    ")\n",
    "\n",
    "print(\"âœ… Context-aware predictor defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4bc321e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(predictions, targets, players_to_predict, num_output_frames):\n",
    "    \"\"\"\n",
    "    Compute evaluation metrics\n",
    "    \"\"\"\n",
    "    metrics = {\n",
    "        'ADE': [],  # Average Displacement Error\n",
    "        'FDE': [],  # Final Displacement Error\n",
    "        'ADE_per_frame': [],\n",
    "        'FDE_per_player': []\n",
    "    }\n",
    "    \n",
    "    batch_size = predictions.shape[0]\n",
    "    \n",
    "    for b in range(batch_size):\n",
    "        T = num_output_frames[b]\n",
    "        player_mask = players_to_predict[b]\n",
    "        \n",
    "        if player_mask.sum() == 0:\n",
    "            continue\n",
    "        \n",
    "        pred = predictions[b, player_mask, :T, :]\n",
    "        target = targets[b, player_mask, :T, :]\n",
    "        \n",
    "        # Compute displacement errors\n",
    "        displacements = torch.sqrt(((pred - target) ** 2).sum(dim=-1))\n",
    "        \n",
    "        # ADE: average over all frames and players\n",
    "        ade = displacements.mean().item()\n",
    "        metrics['ADE'].append(ade)\n",
    "        \n",
    "        # FDE: error at final frame\n",
    "        fde = displacements[:, -1].mean().item()\n",
    "        metrics['FDE'].append(fde)\n",
    "        \n",
    "        # Per-frame ADE\n",
    "        ade_per_frame = displacements.mean(dim=0).cpu().numpy()\n",
    "        metrics['ADE_per_frame'].append(ade_per_frame)\n",
    "        \n",
    "        # Per-player FDE\n",
    "        fde_per_player = displacements[:, -1].cpu().numpy()\n",
    "        metrics['FDE_per_player'].extend(fde_per_player.tolist())\n",
    "    \n",
    "    # Aggregate metrics\n",
    "    summary = {\n",
    "        'mean_ADE': np.mean(metrics['ADE']),\n",
    "        'std_ADE': np.std(metrics['ADE']),\n",
    "        'mean_FDE': np.mean(metrics['FDE']),\n",
    "        'std_FDE': np.std(metrics['FDE']),\n",
    "        'median_ADE': np.median(metrics['ADE']),\n",
    "        'median_FDE': np.median(metrics['FDE'])\n",
    "    }\n",
    "    \n",
    "    return summary, metrics\n",
    "\n",
    "\n",
    "# Evaluate on validation set\n",
    "print(\"\\nðŸ“Š Evaluating model on validation set...\")\n",
    "\n",
    "model.eval()\n",
    "all_metrics = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(val_loader, desc=\"Evaluating\"):\n",
    "        input_seq = batch['input_seq'].to(device)\n",
    "        output_seq = batch['output_seq'].to(device)\n",
    "        mask = batch['mask'].to(device)\n",
    "        role_ids = batch['role_ids'].to(device)\n",
    "        side_ids = batch['side_ids'].to(device)\n",
    "        players_to_predict = batch['players_to_predict']\n",
    "        num_output_frames = batch['num_output_frames']\n",
    "        \n",
    "        # Predict\n",
    "        pred_traj, _ = model(input_seq, role_ids, side_ids, num_output_frames, mask)\n",
    "        \n",
    "        # Apply physics constraints\n",
    "        pred_traj = physics.apply_constraints(pred_traj)\n",
    "        \n",
    "        # Compute metrics\n",
    "        summary, metrics = compute_metrics(\n",
    "            pred_traj.cpu(),\n",
    "            output_seq.cpu(),\n",
    "            players_to_predict,\n",
    "            num_output_frames\n",
    "        )\n",
    "        \n",
    "        all_metrics.append(summary)\n",
    "\n",
    "# Aggregate all metrics\n",
    "final_metrics = {\n",
    "    'ADE': np.mean([m['mean_ADE'] for m in all_metrics]),\n",
    "    'FDE': np.mean([m['mean_FDE'] for m in all_metrics]),\n",
    "    'ADE_std': np.mean([m['std_ADE'] for m in all_metrics]),\n",
    "    'FDE_std': np.mean([m['std_FDE'] for m in all_metrics])\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ðŸ“Š FINAL EVALUATION METRICS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Average Displacement Error (ADE): {final_metrics['ADE']:.4f} Â± {final_metrics['ADE_std']:.4f} yards\")\n",
    "print(f\"Final Displacement Error (FDE):   {final_metrics['FDE']:.4f} Â± {final_metrics['FDE_std']:.4f} yards\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2593577",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_prediction(input_seq, pred_traj, target_traj, metadata, player_idx=0):\n",
    "    \"\"\"\n",
    "    Visualize a single player's trajectory prediction\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(15, 8))\n",
    "    \n",
    "    # Draw field\n",
    "    ax.set_xlim(0, 120)\n",
    "    ax.set_ylim(0, 53.3)\n",
    "    ax.set_xlabel('X (yards)', fontsize=12)\n",
    "    ax.set_ylabel('Y (yards)', fontsize=12)\n",
    "    ax.set_title(f\"Trajectory Prediction - Game {metadata['game_id']}, Play {metadata['play_id']}\", \n",
    "                 fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # Draw field lines\n",
    "    for x in range(0, 121, 10):\n",
    "        ax.axvline(x, color='white', linewidth=0.5, alpha=0.3)\n",
    "    ax.axhline(53.3/2, color='white', linewidth=1, alpha=0.5)\n",
    "    \n",
    "    # Plot input trajectory\n",
    "    input_x = input_seq[:, player_idx, 0].cpu().numpy()\n",
    "    input_y = input_seq[:, player_idx, 1].cpu().numpy()\n",
    "    ax.plot(input_x, input_y, 'o-', color='blue', linewidth=2, \n",
    "            markersize=6, label='Input Trajectory', alpha=0.7)\n",
    "    \n",
    "    # Plot predicted trajectory\n",
    "    pred_x = pred_traj[player_idx, :, 0].cpu().numpy()\n",
    "    pred_y = pred_traj[player_idx, :, 1].cpu().numpy()\n",
    "    ax.plot(pred_x, pred_y, 's-', color='red', linewidth=2, \n",
    "            markersize=6, label='Predicted Trajectory', alpha=0.7)\n",
    "    \n",
    "    # Plot ground truth trajectory\n",
    "    target_x = target_traj[player_idx, :, 0].cpu().numpy()\n",
    "    target_y = target_traj[player_idx, :, 1].cpu().numpy()\n",
    "    ax.plot(target_x, target_y, '^-', color='green', linewidth=2, \n",
    "            markersize=6, label='Ground Truth', alpha=0.7)\n",
    "    \n",
    "    # Mark start and end points\n",
    "    ax.scatter(input_x[0], input_y[0], s=200, c='blue', marker='o', \n",
    "               edgecolors='black', linewidths=2, zorder=5, label='Start')\n",
    "    ax.scatter(target_x[-1], target_y[-1], s=200, c='green', marker='*', \n",
    "               edgecolors='black', linewidths=2, zorder=5, label='End (GT)')\n",
    "    ax.scatter(pred_x[-1], pred_y[-1], s=200, c='red', marker='X', \n",
    "               edgecolors='black', linewidths=2, zorder=5, label='End (Pred)')\n",
    "    \n",
    "    ax.legend(loc='upper right', fontsize=10)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_facecolor('#2d5016')  # Football field green\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "\n",
    "# Visualize some predictions\n",
    "print(\"\\nðŸŽ¨ Creating visualizations...\")\n",
    "\n",
    "# Get a batch\n",
    "sample_batch = next(iter(val_loader))\n",
    "\n",
    "with torch.no_grad():\n",
    "    input_seq = sample_batch['input_seq'].to(device)\n",
    "    output_seq = sample_batch['output_seq']\n",
    "    mask = sample_batch['mask'].to(device)\n",
    "    role_ids = sample_batch['role_ids'].to(device)\n",
    "    side_ids = sample_batch['side_ids'].to(device)\n",
    "    num_output_frames = sample_batch['num_output_frames']\n",
    "    \n",
    "    pred_traj, _ = model(input_seq, role_ids, side_ids, num_output_frames, mask)\n",
    "    pred_traj = physics.apply_constraints(pred_traj)\n",
    "\n",
    "# Visualize first sample\n",
    "fig = visualize_prediction(\n",
    "    input_seq[0].cpu(),\n",
    "    pred_traj[0].cpu(),\n",
    "    output_seq[0],\n",
    "    sample_batch['metadata'][0],\n",
    "    player_idx=0\n",
    ")\n",
    "plt.savefig('trajectory_prediction_sample.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ… Visualization saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c26024",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final model\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'model_config': model_config,\n",
    "    'feature_cols': train_dataset.feature_cols,\n",
    "    'feature_means': train_dataset.feature_means,\n",
    "    'feature_stds': train_dataset.feature_stds,\n",
    "    'final_metrics': final_metrics\n",
    "}, 'final_model_complete.pth')\n",
    "\n",
    "# Save normalization stats\n",
    "normalization_stats = {\n",
    "    'feature_means': train_dataset.feature_means,\n",
    "    'feature_stds': train_dataset.feature_stds,\n",
    "    'feature_cols': train_dataset.feature_cols\n",
    "}\n",
    "\n",
    "with open('normalization_stats.pkl', 'wb') as f:\n",
    "    pickle.dump(normalization_stats, f)\n",
    "\n",
    "# Save metrics\n",
    "with open('evaluation_metrics.json', 'w') as f:\n",
    "    json.dump(final_metrics, f, indent=2)\n",
    "\n",
    "print(\"\\nâœ… All artifacts saved!\")\n",
    "print(\"\\nSaved files:\")\n",
    "print(\"  - final_model_complete.pth\")\n",
    "print(\"  - normalization_stats.pkl\")\n",
    "print(\"  - evaluation_metrics.json\")\n",
    "print(\"  - training_history.pkl\")\n",
    "print(\"  - trajectory_prediction_sample.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
