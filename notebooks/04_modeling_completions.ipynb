{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fff19049",
   "metadata": {},
   "source": [
    "# Modeling Completions:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb4b6ec",
   "metadata": {},
   "source": [
    "- Predicts whether a pass will be completed (C), incomplete (I), or intercepted (IN)\n",
    "- Uses similar features from the route modeling approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73a72f62",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mseaborn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msns\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpathlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpickle\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jrzem\\Downloads\\.venv\\Lib\\site-packages\\seaborn\\__init__.py:5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *  \u001b[38;5;66;03m# noqa: F401,F403\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpalettes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *  \u001b[38;5;66;03m# noqa: F401,F403\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mrelational\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *  \u001b[38;5;66;03m# noqa: F401,F403\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mregression\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *  \u001b[38;5;66;03m# noqa: F401,F403\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcategorical\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *  \u001b[38;5;66;03m# noqa: F401,F403\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jrzem\\Downloads\\.venv\\Lib\\site-packages\\seaborn\\relational.py:21\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     14\u001b[39m     adjust_legend_subtitles,\n\u001b[32m     15\u001b[39m     _default_color,\n\u001b[32m   (...)\u001b[39m\u001b[32m     18\u001b[39m     _scatter_legend_artist,\n\u001b[32m     19\u001b[39m )\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_compat\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m groupby_apply_include_groups\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_statistics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m EstimateAggregator, WeightedAggregator\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01maxisgrid\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FacetGrid, _facet_docs\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_docstrings\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DocstringComponents, _core_docs\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jrzem\\Downloads\\.venv\\Lib\\site-packages\\seaborn\\_statistics.py:32\u001b[39m\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mstats\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m gaussian_kde\n\u001b[32m     33\u001b[39m     _no_scipy = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jrzem\\Downloads\\.venv\\Lib\\site-packages\\scipy\\stats\\__init__.py:626\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[33;03m.. _statsrefmanual:\u001b[39;00m\n\u001b[32m      3\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    621\u001b[39m \n\u001b[32m    622\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m  \u001b[38;5;66;03m# noqa: E501\u001b[39;00m\n\u001b[32m    624\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_warnings_errors\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (ConstantInputWarning, NearConstantInputWarning,\n\u001b[32m    625\u001b[39m                                DegenerateDataWarning, FitError)\n\u001b[32m--> \u001b[39m\u001b[32m626\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_stats_py\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m    627\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_variation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m variation\n\u001b[32m    628\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdistributions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jrzem\\Downloads\\.venv\\Lib\\site-packages\\scipy\\stats\\_stats_py.py:40\u001b[39m\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m array, asarray, ma\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m sparse\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mspatial\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m distance_matrix\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01moptimize\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m milp, LinearConstraint\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_lib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_util\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (check_random_state, _get_nan,\n\u001b[32m     44\u001b[39m                               _rename_parameter, _contains_nan,\n\u001b[32m     45\u001b[39m                               normalize_axis_index, np_vecdot, AxisError)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jrzem\\Downloads\\.venv\\Lib\\site-packages\\scipy\\spatial\\__init__.py:123\u001b[39m\n\u001b[32m    119\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ckdtree, kdtree, qhull\n\u001b[32m    121\u001b[39m __all__ = [s \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mdir\u001b[39m() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m s.startswith(\u001b[33m'\u001b[39m\u001b[33m_\u001b[39m\u001b[33m'\u001b[39m)]\n\u001b[32m--> \u001b[39m\u001b[32m123\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m distance, transform\n\u001b[32m    125\u001b[39m __all__ += [\u001b[33m'\u001b[39m\u001b[33mdistance\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mtransform\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m    127\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_lib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_testutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PytestTester\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jrzem\\Downloads\\.venv\\Lib\\site-packages\\scipy\\spatial\\transform\\__init__.py:20\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[33;03mSpatial Transformations (:mod:`scipy.spatial.transform`)\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[33;03m========================================================\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     18\u001b[39m \u001b[33;03m   RotationSpline\u001b[39;00m\n\u001b[32m     19\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_rigid_transform\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RigidTransform\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_rotation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Rotation, Slerp\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_rotation_spline\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RotationSpline\n",
      "\u001b[36mFile \u001b[39m\u001b[32mscipy/spatial/transform/_rigid_transform.pyx:4\u001b[39m, in \u001b[36minit scipy.spatial.transform._rigid_transform\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:645\u001b[39m, in \u001b[36mparent\u001b[39m\u001b[34m(self)\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (classification_report, confusion_matrix, \n",
    "                             accuracy_score, precision_recall_fscore_support,\n",
    "                             roc_auc_score, roc_curve)\n",
    "import xgboost as xgb\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Paths\n",
    "RAW_DATA_PATH = Path(\"../data/raw\")\n",
    "PROCESSED_DATA_PATH = Path(\"../data/processed\")\n",
    "MODELS_PATH = Path(\"../models\")\n",
    "MODELS_PATH.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "print(\"‚úÖ Setup complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e784e8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data with Enhanced Features\n",
    "print(\"üìÇ Loading enhanced feature data...\")\n",
    "\n",
    "# Load all weeks\n",
    "weeks = ['01', '02', '03', '04', '05', '06', '07', '08', '09', \n",
    "         '10', '11', '12', '13', '14', '15', '16', '17', '18']\n",
    "\n",
    "all_data = []\n",
    "for week in weeks:\n",
    "    parquet_path = PROCESSED_DATA_PATH / f\"features_enhanced_w{week}.parquet\"\n",
    "    if parquet_path.exists():\n",
    "        df = pd.read_parquet(parquet_path)\n",
    "        all_data.append(df)\n",
    "        print(f\"  ‚úì Loaded Week {week}\")\n",
    "    else:\n",
    "        print(f\"  ‚ö†Ô∏è Missing: Week {week}\")\n",
    "\n",
    "df = pd.concat(all_data, ignore_index=True)\n",
    "print(f\"\\\\n‚úÖ Loaded {len(df):,} rows from {len(all_data)} weeks\")\n",
    "print(f\"   {df['game_id'].nunique()} games, {df[['game_id', 'play_id']].drop_duplicates().shape[0]} plays\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3f4a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Play-Level Dataset for Completion Prediction\n",
    "print(\"\\\\n\" + \"=\"*80)\n",
    "print(\"CREATING PLAY-LEVEL COMPLETION DATASET\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# We need one row per play with aggregated features\n",
    "# Key: We'll aggregate features at the snap (last frame before throw)\n",
    "\n",
    "def create_completion_dataset(df):\n",
    "    \"\"\"\n",
    "    Create play-level dataset for completion prediction\n",
    "    Features are aggregated from the last frame (snap) for each player\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get the last frame for each play (snap moment)\n",
    "    last_frames = df.groupby(['game_id', 'play_id'])['frame_id'].max().reset_index()\n",
    "    last_frames.columns = ['game_id', 'play_id', 'last_frame']\n",
    "    \n",
    "    # Merge to get snap data\n",
    "    snap_data = df.merge(last_frames, on=['game_id', 'play_id'])\n",
    "    snap_data = snap_data[snap_data['frame_id'] == snap_data['last_frame']]\n",
    "    \n",
    "    # Create play-level features\n",
    "    play_features = []\n",
    "    \n",
    "    for (game_id, play_id), group in snap_data.groupby(['game_id', 'play_id']):\n",
    "        features = {}\n",
    "        features['game_id'] = game_id\n",
    "        features['play_id'] = play_id\n",
    "        \n",
    "        # Target variable\n",
    "        if 'pass_result' in group.columns:\n",
    "            features['pass_result'] = group['pass_result'].iloc[0]\n",
    "        \n",
    "        # === QUARTERBACK FEATURES ===\n",
    "        qb = group[group['player_role'] == 'Passer']\n",
    "        if len(qb) > 0:\n",
    "            features['qb_x'] = qb['x'].iloc[0]\n",
    "            features['qb_y'] = qb['y'].iloc[0]\n",
    "            features['qb_speed'] = qb['s'].iloc[0]\n",
    "            features['qb_acceleration'] = qb['a'].iloc[0]\n",
    "            features['qb_orientation'] = qb['o'].iloc[0]\n",
    "            features['qb_direction'] = qb['dir'].iloc[0]\n",
    "            if 's_smooth' in qb.columns:\n",
    "                features['qb_speed_smooth'] = qb['s_smooth'].iloc[0]\n",
    "            if 'dist_to_nearest_sideline' in qb.columns:\n",
    "                features['qb_dist_to_sideline'] = qb['dist_to_nearest_sideline'].iloc[0]\n",
    "        \n",
    "        # === TARGETED RECEIVER FEATURES ===\n",
    "        target = group[group['player_role'] == 'Targeted Receiver']\n",
    "        if len(target) > 0:\n",
    "            features['target_x'] = target['x'].iloc[0]\n",
    "            features['target_y'] = target['y'].iloc[0]\n",
    "            features['target_speed'] = target['s'].iloc[0]\n",
    "            features['target_acceleration'] = target['a'].iloc[0]\n",
    "            features['target_direction'] = target['dir'].iloc[0]\n",
    "            if 's_smooth' in target.columns:\n",
    "                features['target_speed_smooth'] = target['s_smooth'].iloc[0]\n",
    "            if 'dist_to_ball_land' in target.columns:\n",
    "                features['target_dist_to_ball_land'] = target['dist_to_ball_land'].iloc[0]\n",
    "            if 'receiver_separation' in target.columns:\n",
    "                features['receiver_separation'] = target['receiver_separation'].iloc[0]\n",
    "            if 'nearest_opponent_dist' in target.columns:\n",
    "                features['target_nearest_defender'] = target['nearest_opponent_dist'].iloc[0]\n",
    "            \n",
    "            # Distance between QB and target\n",
    "            if len(qb) > 0:\n",
    "                features['qb_target_distance'] = np.sqrt(\n",
    "                    (features['qb_x'] - features['target_x'])**2 + \n",
    "                    (features['qb_y'] - features['target_y'])**2\n",
    "                )\n",
    "        \n",
    "        # === DEFENSIVE COVERAGE FEATURES ===\n",
    "        defenders = group[group['player_side'] == 'Defense']\n",
    "        if len(defenders) > 0:\n",
    "            features['num_defenders'] = len(defenders)\n",
    "            features['avg_defender_speed'] = defenders['s'].mean()\n",
    "            if 'nearest_opponent_dist' in defenders.columns:\n",
    "                features['avg_defender_separation'] = defenders['nearest_opponent_dist'].mean()\n",
    "                features['min_defender_separation'] = defenders['nearest_opponent_dist'].min()\n",
    "            if 'player_density_5yd' in defenders.columns:\n",
    "                features['avg_defender_density'] = defenders['player_density_5yd'].mean()\n",
    "        \n",
    "        # === OFFENSIVE FEATURES ===\n",
    "        offense = group[group['player_side'] == 'Offense']\n",
    "        if len(offense) > 0:\n",
    "            features['num_offensive_players'] = len(offense)\n",
    "            features['avg_offensive_speed'] = offense['s'].mean()\n",
    "        \n",
    "        # === PLAY CONTEXT FEATURES ===\n",
    "        # These are the same for all players in a play\n",
    "        context_cols = [\n",
    "            'down', 'yards_to_go', 'quarter', 'absolute_yardline_number',\n",
    "            'score_differential', 'field_position_norm', 'yards_to_go_norm',\n",
    "            'defenders_in_the_box', 'pass_length', 'dropback_distance',\n",
    "            'pre_snap_home_score', 'pre_snap_visitor_score'\n",
    "        ]\n",
    "        \n",
    "        for col in context_cols:\n",
    "            if col in group.columns:\n",
    "                features[col] = group[col].iloc[0]\n",
    "        \n",
    "        # === CATEGORICAL FEATURES ===\n",
    "        categorical_cols = [\n",
    "            'team_coverage_man_zone', 'team_coverage_type', 'dropback_type',\n",
    "            'route_of_targeted_receiver', 'offense_formation', 'receiver_alignment',\n",
    "            'play_action', 'pass_location_type'\n",
    "        ]\n",
    "        \n",
    "        for col in categorical_cols:\n",
    "            if col in group.columns:\n",
    "                features[col] = group[col].iloc[0]\n",
    "        \n",
    "        # === ENHANCED FEATURES (if available) ===\n",
    "        enhanced_cols = [\n",
    "            'two_minute_drill', 'red_zone', 'third_down_long', 'passing_down',\n",
    "            'is_close_game', 'high_pressure_situation', 'route_completion_rate',\n",
    "            'is_quick_release', 'is_deep_route'\n",
    "        ]\n",
    "        \n",
    "        for col in enhanced_cols:\n",
    "            if col in group.columns:\n",
    "                features[col] = group[col].iloc[0]\n",
    "        \n",
    "        play_features.append(features)\n",
    "    \n",
    "    return pd.DataFrame(play_features)\n",
    "\n",
    "# Create the dataset\n",
    "print(\"\\\\n‚è≥ Creating play-level completion dataset...\")\n",
    "completion_df = create_completion_dataset(df)\n",
    "\n",
    "print(f\"\\\\n‚úÖ Created completion dataset:\")\n",
    "print(f\"   Shape: {completion_df.shape}\")\n",
    "print(f\"   Plays: {len(completion_df):,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ee82a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore Target Variable\n",
    "print(\"\\\\n\" + \"=\"*80)\n",
    "print(\"TARGET VARIABLE ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Check pass_result distribution\n",
    "if 'pass_result' in completion_df.columns:\n",
    "    print(\"\\\\nPass Result Distribution:\")\n",
    "    result_counts = completion_df['pass_result'].value_counts()\n",
    "    print(result_counts)\n",
    "    print(f\"\\\\nPercentages:\")\n",
    "    print(result_counts / len(completion_df) * 100)\n",
    "    \n",
    "    # Visualize\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Count plot\n",
    "    result_counts.plot(kind='bar', ax=axes[0], color=['green', 'red', 'orange', 'blue', 'purple'])\n",
    "    axes[0].set_title('Pass Result Distribution', fontsize=14, fontweight='bold')\n",
    "    axes[0].set_xlabel('Pass Result')\n",
    "    axes[0].set_ylabel('Count')\n",
    "    axes[0].grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Pie chart\n",
    "    axes[1].pie(result_counts.values, labels=result_counts.index, autopct='%1.1f%%',\n",
    "                colors=['green', 'red', 'orange', 'blue', 'purple'])\n",
    "    axes[1].set_title('Pass Result Proportions', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(MODELS_PATH / 'pass_result_distribution.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è pass_result column not found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e3f469",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering for Completion Prediction\n",
    "print(\"\\\\n\" + \"=\"*80)\n",
    "print(\"FEATURE ENGINEERING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create binary completion target (simplify to Complete vs Not Complete)\n",
    "completion_df['is_complete'] = (completion_df['pass_result'] == 'C').astype(int)\n",
    "\n",
    "print(f\"\\\\nBinary Target Distribution:\")\n",
    "print(completion_df['is_complete'].value_counts())\n",
    "print(f\"\\\\nCompletion Rate: {completion_df['is_complete'].mean():.2%}\")\n",
    "\n",
    "# Select features for modeling\n",
    "numeric_features = [\n",
    "    # QB features\n",
    "    'qb_speed', 'qb_acceleration', 'qb_dist_to_sideline',\n",
    "    # Target features\n",
    "    'target_speed', 'target_acceleration', 'target_dist_to_ball_land',\n",
    "    'receiver_separation', 'target_nearest_defender', 'qb_target_distance',\n",
    "    # Defensive features\n",
    "    'num_defenders', 'avg_defender_speed', 'avg_defender_separation',\n",
    "    'min_defender_separation', 'avg_defender_density',\n",
    "    # Play context\n",
    "    'down', 'yards_to_go', 'absolute_yardline_number', 'score_differential',\n",
    "    'defenders_in_the_box', 'pass_length', 'dropback_distance'\n",
    "]\n",
    "\n",
    "categorical_features = [\n",
    "    'team_coverage_man_zone', 'team_coverage_type', 'dropback_type',\n",
    "    'route_of_targeted_receiver', 'offense_formation', 'play_action'\n",
    "]\n",
    "\n",
    "# Filter to available features\n",
    "numeric_features = [f for f in numeric_features if f in completion_df.columns]\n",
    "categorical_features = [f for f in categorical_features if f in completion_df.columns]\n",
    "\n",
    "print(f\"\\\\nSelected Features:\")\n",
    "print(f\"  Numeric: {len(numeric_features)}\")\n",
    "print(f\"  Categorical: {len(categorical_features)}\")\n",
    "print(f\"  Total: {len(numeric_features) + len(categorical_features)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ea36b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Data for Modeling\n",
    "print(\"\\\\n\" + \"=\"*80)\n",
    "print(\"DATA PREPARATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create feature matrix\n",
    "X_numeric = completion_df[numeric_features].fillna(0)\n",
    "X_categorical = completion_df[categorical_features].fillna('Unknown')\n",
    "\n",
    "# Encode categorical features\n",
    "label_encoders = {}\n",
    "X_categorical_encoded = pd.DataFrame()\n",
    "\n",
    "for col in categorical_features:\n",
    "    le = LabelEncoder()\n",
    "    X_categorical_encoded[col] = le.fit_transform(X_categorical[col].astype(str))\n",
    "    label_encoders[col] = le\n",
    "\n",
    "# Combine features\n",
    "X = pd.concat([X_numeric, X_categorical_encoded], axis=1)\n",
    "y = completion_df['is_complete']\n",
    "\n",
    "print(f\"\\\\nFeature Matrix Shape: {X.shape}\")\n",
    "print(f\"Target Shape: {y.shape}\")\n",
    "print(f\"\\\\nFeature Names ({len(X.columns)}):\")\n",
    "for i, col in enumerate(X.columns, 1):\n",
    "    print(f\"  {i}. {col}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61846f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-Test Split\n",
    "print(\"\\\\n\" + \"=\"*80)\n",
    "print(\"TRAIN-TEST SPLIT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Stratified split to maintain class balance\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"\\\\nTrain Set: {X_train.shape[0]:,} samples\")\n",
    "print(f\"  Complete: {y_train.sum():,} ({y_train.mean():.2%})\")\n",
    "print(f\"  Incomplete: {(~y_train.astype(bool)).sum():,} ({(1-y_train.mean()):.2%})\")\n",
    "\n",
    "print(f\"\\\\nTest Set: {X_test.shape[0]:,} samples\")\n",
    "print(f\"  Complete: {y_test.sum():,} ({y_test.mean():.2%})\")\n",
    "print(f\"  Incomplete: {(~y_test.astype(bool)).sum():,} ({(1-y_test.mean()):.2%})\")\n",
    "\n",
    "# Normalize features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"\\\\n‚úÖ Features normalized\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8187029b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle Class Imbalance with SMOTE (Optional)\n",
    "print(\"\\\\n\" + \"=\"*80)\n",
    "print(\"HANDLING CLASS IMBALANCE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Check if classes are imbalanced\n",
    "class_ratio = y_train.value_counts()[1] / y_train.value_counts()[0]\n",
    "print(f\"\\\\nClass Ratio (Complete/Incomplete): {class_ratio:.2f}\")\n",
    "\n",
    "if class_ratio < 0.8 or class_ratio > 1.2:\n",
    "    print(\"\\\\n‚ö†Ô∏è Classes are imbalanced. Applying SMOTE...\")\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_train_balanced, y_train_balanced = smote.fit_resample(X_train_scaled, y_train)\n",
    "    \n",
    "    print(f\"\\\\nBalanced Train Set: {X_train_balanced.shape[0]:,} samples\")\n",
    "    print(f\"  Complete: {y_train_balanced.sum():,} ({y_train_balanced.mean():.2%})\")\n",
    "    print(f\"  Incomplete: {(~y_train_balanced.astype(bool)).sum():,} ({(1-y_train_balanced.mean()):.2%})\")\n",
    "else:\n",
    "    print(\"\\\\n‚úÖ Classes are balanced. No SMOTE needed.\")\n",
    "    X_train_balanced = X_train_scaled\n",
    "    y_train_balanced = y_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f733cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Training - Multiple Models\n",
    "print(\"\\\\n\" + \"=\"*80)\n",
    "print(\"MODEL TRAINING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42, n_jobs=-1),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, max_depth=5, random_state=42),\n",
    "    'XGBoost': xgb.XGBClassifier(n_estimators=100, max_depth=5, random_state=42, eval_metric='logloss')\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\\\n{'='*60}\")\n",
    "    print(f\"Training {name}...\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Train\n",
    "    model.fit(X_train_balanced, y_train_balanced)\n",
    "    \n",
    "    # Predict\n",
    "    y_pred_train = model.predict(X_train_scaled)\n",
    "    y_pred_test = model.predict(X_test_scaled)\n",
    "    y_pred_proba_test = model.predict_proba(X_test_scaled)[:, 1]\n",
    "    \n",
    "    # Evaluate\n",
    "    train_acc = accuracy_score(y_train, y_pred_train)\n",
    "    test_acc = accuracy_score(y_test, y_pred_test)\n",
    "    \n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred_test, average='binary')\n",
    "    \n",
    "    try:\n",
    "        auc = roc_auc_score(y_test, y_pred_proba_test)\n",
    "    except:\n",
    "        auc = None\n",
    "    \n",
    "    results[name] = {\n",
    "        'model': model,\n",
    "        'train_acc': train_acc,\n",
    "        'test_acc': test_acc,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'auc': auc,\n",
    "        'y_pred': y_pred_test,\n",
    "        'y_pred_proba': y_pred_proba_test\n",
    "    }\n",
    "    \n",
    "    print(f\"\\\\nüìä Results:\")\n",
    "    print(f\"  Train Accuracy: {train_acc:.4f}\")\n",
    "    print(f\"  Test Accuracy:  {test_acc:.4f}\")\n",
    "    print(f\"  Precision:      {precision:.4f}\")\n",
    "    print(f\"  Recall:         {recall:.4f}\")\n",
    "    print(f\"  F1 Score:       {f1:.4f}\")\n",
    "    if auc:\n",
    "        print(f\"  AUC-ROC:        {auc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d76a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Comparison\n",
    "print(\"\\\\n\" + \"=\"*80)\n",
    "print(\"MODEL COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Model': list(results.keys()),\n",
    "    'Train Acc': [r['train_acc'] for r in results.values()],\n",
    "    'Test Acc': [r['test_acc'] for r in results.values()],\n",
    "    'Precision': [r['precision'] for r in results.values()],\n",
    "    'Recall': [r['recall'] for r in results.values()],\n",
    "    'F1 Score': [r['f1'] for r in results.values()],\n",
    "    'AUC-ROC': [r['auc'] if r['auc'] else 0 for r in results.values()]\n",
    "})\n",
    "\n",
    "print(\"\\\\n\", comparison_df.to_string(index=False))\n",
    "\n",
    "# Find best model\n",
    "best_model_name = comparison_df.loc[comparison_df['F1 Score'].idxmax(), 'Model']\n",
    "best_model = results[best_model_name]['model']\n",
    "\n",
    "print(f\"\\\\nüèÜ Best Model: {best_model_name}\")\n",
    "print(f\"   F1 Score: {results[best_model_name]['f1']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63e1eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Model Comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "metrics = ['Test Acc', 'Precision', 'Recall', 'F1 Score']\n",
    "for idx, metric in enumerate(metrics):\n",
    "    ax = axes[idx // 2, idx % 2]\n",
    "    comparison_df.plot(x='Model', y=metric, kind='bar', ax=ax, legend=False, color='steelblue')\n",
    "    ax.set_title(f'{metric} Comparison', fontsize=12, fontweight='bold')\n",
    "    ax.set_ylabel(metric)\n",
    "    ax.set_xlabel('')\n",
    "    ax.set_xticklabels(comparison_df['Model'], rotation=45, ha='right')\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    ax.set_ylim([0, 1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(MODELS_PATH / 'model_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036c8195",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix for Best Model\n",
    "print(\"\\\\n\" + \"=\"*80)\n",
    "print(f\"CONFUSION MATRIX - {best_model_name}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "y_pred_best = results[best_model_name]['y_pred']\n",
    "cm = confusion_matrix(y_test, y_pred_best)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax,\n",
    "            xticklabels=['Incomplete', 'Complete'],\n",
    "            yticklabels=['Incomplete', 'Complete'])\n",
    "ax.set_title(f'Confusion Matrix - {best_model_name}', fontsize=14, fontweight='bold')\n",
    "ax.set_ylabel('True Label')\n",
    "ax.set_xlabel('Predicted Label')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(MODELS_PATH / f'confusion_matrix_{best_model_name.replace(\" \", \"_\")}.png', \n",
    "            dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_best, target_names=['Incomplete', 'Complete']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa99f6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curve for Best Model\n",
    "if results[best_model_name]['auc']:\n",
    "    print(\"\\\\n\" + \"=\"*80)\n",
    "    print(f\"ROC CURVE - {best_model_name}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    y_pred_proba_best = results[best_model_name]['y_pred_proba']\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba_best)\n",
    "    auc = results[best_model_name]['auc']\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    ax.plot(fpr, tpr, linewidth=2, label=f'ROC Curve (AUC = {auc:.3f})')\n",
    "    ax.plot([0, 1], [0, 1], 'k--', linewidth=1, label='Random Classifier')\n",
    "    ax.set_xlabel('False Positive Rate', fontsize=12)\n",
    "    ax.set_ylabel('True Positive Rate', fontsize=12)\n",
    "    ax.set_title(f'ROC Curve - {best_model_name}', fontsize=14, fontweight='bold')\n",
    "    ax.legend(loc='lower right')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(MODELS_PATH / f'roc_curve_{best_model_name.replace(\" \", \"_\")}.png', \n",
    "                dpi=300, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc9338c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance (for tree-based models)\n",
    "if hasattr(best_model, 'feature_importances_'):\n",
    "    print(\"\\\\n\" + \"=\"*80)\n",
    "    print(f\"FEATURE IMPORTANCE - {best_model_name}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    feature_importance = pd.DataFrame({\n",
    "        'Feature': X.columns,\n",
    "        'Importance': best_model.feature_importances_\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "    \n",
    "    print(\"\\\\nTop 20 Most Important Features:\")\n",
    "    print(feature_importance.head(20).to_string(index=False))\n",
    "    \n",
    "    # Visualize\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    top_features = feature_importance.head(20)\n",
    "    ax.barh(range(len(top_features)), top_features['Importance'], color='steelblue')\n",
    "    ax.set_yticks(range(len(top_features)))\n",
    "    ax.set_yticklabels(top_features['Feature'])\n",
    "    ax.set_xlabel('Importance', fontsize=12)\n",
    "    ax.set_title(f'Top 20 Feature Importances - {best_model_name}', \n",
    "                 fontsize=14, fontweight='bold')\n",
    "    ax.invert_yaxis()\n",
    "    ax.grid(True, alpha=0.3, axis='x')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(MODELS_PATH / f'feature_importance_{best_model_name.replace(\" \", \"_\")}.png', \n",
    "                dpi=300, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba6850d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Best Model and Artifacts\n",
    "print(\"\\\\n\" + \"=\"*80)\n",
    "print(\"SAVING MODEL AND ARTIFACTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Save model\n",
    "model_path = MODELS_PATH / f'completion_model_{best_model_name.replace(\" \", \"_\")}.pkl'\n",
    "with open(model_path, 'wb') as f:\n",
    "    pickle.dump(best_model, f)\n",
    "print(f\"\\\\n‚úÖ Model saved: {model_path}\")\n",
    "\n",
    "# Save scaler\n",
    "scaler_path = MODELS_PATH / 'completion_scaler.pkl'\n",
    "with open(scaler_path, 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "print(f\"‚úÖ Scaler saved: {scaler_path}\")\n",
    "\n",
    "# Save label encoders\n",
    "encoders_path = MODELS_PATH / 'completion_label_encoders.pkl'\n",
    "with open(encoders_path, 'wb') as f:\n",
    "    pickle.dump(label_encoders, f)\n",
    "print(f\"‚úÖ Label encoders saved: {encoders_path}\")\n",
    "\n",
    "# Save feature names\n",
    "features_path = MODELS_PATH / 'completion_features.pkl'\n",
    "with open(features_path, 'wb') as f:\n",
    "    pickle.dump({\n",
    "        'numeric_features': numeric_features,\n",
    "        'categorical_features': categorical_features,\n",
    "        'all_features': list(X.columns)\n",
    "    }, f)\n",
    "print(f\"‚úÖ Feature names saved: {features_path}\")\n",
    "\n",
    "# Save results\n",
    "results_path = MODELS_PATH / 'completion_model_results.pkl'\n",
    "with open(results_path, 'wb') as f:\n",
    "    pickle.dump({\n",
    "        'comparison': comparison_df,\n",
    "        'best_model_name': best_model_name,\n",
    "        'results': {k: {kk: vv for kk, vv in v.items() if kk != 'model'} \n",
    "                   for k, v in results.items()}\n",
    "    }, f)\n",
    "print(f\"‚úÖ Results saved: {results_path}\")\n",
    "\n",
    "print(\"\\\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ COMPLETION PREDICTION MODEL COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\\\nBest Model: {best_model_name}\")\n",
    "print(f\"Test Accuracy: {results[best_model_name]['test_acc']:.4f}\")\n",
    "print(f\"F1 Score: {results[best_model_name]['f1']:.4f}\")\n",
    "print(f\"AUC-ROC: {results[best_model_name]['auc']:.4f}\")\n",
    "print(\"\\\\nüìÅ All artifacts saved to:\", MODELS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd52911",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-Validation Analysis (Bonus)\n",
    "print(\"\\\\n\" + \"=\"*80)\n",
    "print(\"CROSS-VALIDATION ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\\\n{name}:\")\n",
    "    cv_scores = cross_val_score(model, X_train_balanced, y_train_balanced, \n",
    "                                 cv=cv, scoring='f1', n_jobs=-1)\n",
    "    print(f\"  CV F1 Scores: {cv_scores}\")\n",
    "    print(f\"  Mean F1: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")\n",
    "\n",
    "print(\"\\\\n‚úÖ Cross-validation complete\")\n",
    "'''\n",
    "\n",
    "print(\"  ‚Ä¢ Creates play-level features from tracking data\")\n",
    "print(\"  ‚Ä¢ Predicts pass completion (Complete vs Incomplete)\")\n",
    "print(\"  ‚Ä¢ Uses similar features from route modeling\")\n",
    "print(\"  ‚Ä¢ Trains multiple models (Logistic Regression, Random Forest, Gradient Boosting, XGBoost)\")\n",
    "print(\"  ‚Ä¢ Handles class imbalance with SMOTE\")\n",
    "print(\"  ‚Ä¢ Provides comprehensive evaluation metrics\")\n",
    "print(\"  ‚Ä¢ Saves best model and artifacts\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
